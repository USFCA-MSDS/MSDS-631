{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "\n",
        "\n",
        "\n",
        "*   Introduce dense layers mathematically\n",
        "*   Introduce dense layers with pytorch\n",
        "* Build a model using dense layers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dense Layers and multi layer perceptrons (MLPs) \n",
        "\n",
        "In neural networks, dense layers (also known as fully connected layers) are the most common type of layer used for building deep learning models. A dense layer is a type of layer where every neuron in the layer is connected to every neuron in the previous layer. This means that each input feature is connected to every neuron in the layer, and each neuron in the layer contributes to the output of every subsequent layer.\n",
        "\n",
        "Dense layers are used for transforming inputs into a higher dimensional representation, allowing for more complex models to be learned. They are called \"dense\" because each neuron in the layer is densely connected to every neuron in the previous layer.\n",
        "\n",
        "In a dense layer, the output of each neuron is computed as a weighted sum of the inputs, followed by a non-linear activation function. The weights in the layer are learned during the training process, and are adjusted to minimize the error between the predicted output and the actual output.\n",
        "\n",
        "Dense layers are often stacked together in deep neural networks, with each layer learning increasingly complex features from the input data.\n",
        "\n",
        "## Mathematically \n",
        "\n",
        "\n",
        "Mathematiaclly a dense layer can be written as\n",
        "\n",
        "$y = f(Wx + b)$\n",
        "\n",
        "where y is the output vector, x is the input vector, W is the weight matrix (also called trainable parameter) of size (m x n), b is the bias vector of size m (also trainable), and f() is the activation function applied element-wise to the matrix-vector product. For instance f can be the function $Relu$ which is defined to be $Relu(x)=0$ if $x\\geq 0$ and zero otherwise.\n",
        "\n",
        "\n",
        "\n",
        "## MLP, a very simple example :"
      ],
      "metadata": {
        "id": "wVXf31-fBa7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the context of PyTorch, MLP refers to a multilayer perceptron model implemented using the PyTorch framework. Recall that PyTorch is a popular deep learning library that provides tools and functionalities for building and training neural networks.\n",
        "\n",
        "In PyTorch, an MLP is typically constructed by combining multiple layers, including linear layers (also known as fully connected layers) and activation functions. The linear layer in PyTorch is implemented using the torch.nn.Linear class. It represents a fully connected layer in which each neuron is connected to every neuron in the previous and next layers.\n",
        "\n",
        "The torch.nn.Linear class takes two parameters: the number of input features and the number of output features. These parameters define the shape of the weight matrix that determines the connections and weights between the neurons. The input features correspond to the size of the previous layer, and the output features correspond to the size of the current layer.\n",
        "\n",
        "Here's an example of how you can define an MLP using linear layers in PyTorch:\n",
        "\n"
      ],
      "metadata": {
        "id": "Ab8zOtNSfhka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input and output sizes\n",
        "input_size = 10\n",
        "output_size = 5\n",
        "\n",
        "# Define a simple dense neural network with one hidden layer\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 20)  # 20 hidden units \n",
        "        self.fc2 = nn.Linear(20, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # activation function for the hidden layer --this computes relu( W x +b ) described above mathematically\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MLP()\n",
        "\n",
        "# Generate some random input data\n",
        "x = torch.randn(32, input_size) # so you should think about x as a vector of dimension 10 and we have 32 sample points of it.\n",
        "\n",
        "# Feed the input through the model to generate output\n",
        "output = model(x)\n",
        "print(output.shape)  # should be (32, 5) since we have 32 samples and 5 output classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqD_sFTOD4hX",
        "outputId": "e54625be-f110-4804-d28a-62d5e6d023c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the MLP consists of two linear layers (fc1 and fc2). The input size of fc1 is input_size, and the output size is hidden_size. The input size of fc2 is hidden_size, and the output size is output_size. The torch.relu function is used as the activation function applied to the output of fc1, and no activation function is applied to the output of fc2.\n",
        "\n",
        "By stacking multiple linear layers with activation functions, an MLP in PyTorch can learn complex patterns and relationships in the data."
      ],
      "metadata": {
        "id": "DHlx8II6x7TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP \n",
        "\n",
        "In the context of PyTorch, MLP refers to a multilayer perceptron model implemented using the PyTorch framework. PyTorch is a popular deep learning library that provides tools and functionalities for building and training neural networks.\n",
        "\n",
        "In PyTorch, an MLP is typically constructed by combining multiple layers, including linear layers (also known as fully connected layers) and activation functions. The linear layer in PyTorch is implemented using the torch.nn.Linear class. It represents a fully connected layer in which each neuron is connected to every neuron in the previous and next layers.\n",
        "\n",
        "The torch.nn.Linear class takes two parameters: the number of input features and the number of output features. These parameters define the shape of the weight matrix that determines the connections and weights between the neurons. The input features correspond to the size of the previous layer, and the output features correspond to the size of the current layer.\n",
        "\n",
        "Here's an example of how you can define an MLP using linear layers in PyTorch:"
      ],
      "metadata": {
        "id": "qoLI0BLaxw1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Excercise : A more realistic example with MLP.\n",
        "\n",
        "# lets get the data\n",
        "\n",
        "get the data https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
        "\n",
        "\n",
        "Using the NumPy function loadtxt(), you can load the file as a matrix of numerical values. The dataset consists of eight input variables and one output variable, which is the last column. The objective is to create a model that maps rows of input variables to an output variable, commonly referred to as a binary classification problem. The input variables are as follows:\n",
        "\n",
        "* Number of times pregnant\n",
        "* Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
        "* Diastolic blood pressure (mm Hg)\n",
        "* Triceps skin fold thickness (mm)\n",
        "* 2-hour serum insulin (μIU/ml)\n",
        "* Body mass index (weight in kg/(height in m)2)\n",
        "* Diabetes pedigree function\n",
        "* Age (years)\n",
        "\n",
        "The output variable is a binary class label (0 or 1). Once the CSV file is loaded into memory, you can divide the columns of data into input and output variables. The data will be stored as a 2D array where the first dimension represents the rows and the second dimension represents the columns, for example, (rows, columns). You can divide the array into two arrays by selecting subsets of columns using the standard NumPy slice operator “:”. The first eight columns can be selected by using the slice 0:8, and the output column can be selected by using index 8."
      ],
      "metadata": {
        "id": "1nOlyjMLEATX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# load the dataset, split into input (X) and output (y) variables\n",
        "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
        "X = dataset[:,0:8]\n",
        "y = dataset[:,8]\n",
        " \n"
      ],
      "metadata": {
        "id": "MnnHuFdmBcRn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now lets convert the above data to pytorch tensors."
      ],
      "metadata": {
        "id": "nHvbCjS_B_VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "ZwR4Ss2TCF82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIdYlMJ_CVoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "owxo_PxICWH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "TOoNZsgaCWaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the model\n",
        "class PimaClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # define the layers of the neural network\n",
        "        self.hidden1 = nn.Linear(8, 12)  # input layer\n",
        "        self.act1 = nn.ReLU()  # activation function for hidden layer 1\n",
        "        self.hidden2 = nn.Linear(12, 8)  # hidden layer 2\n",
        "        self.act2 = nn.ReLU()  # activation function for hidden layer 2\n",
        "        self.output = nn.Linear(8, 1)  # output layer\n",
        "        self.act_output = nn.Sigmoid()  # activation function for the output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward pass of the neural network\n",
        "        x = self.act1(self.hidden1(x))  # pass input through hidden layer 1\n",
        "        x = self.act2(self.hidden2(x))  # pass output of hidden layer 1 through hidden layer 2\n",
        "        x = self.act_output(self.output(x))  # pass output of hidden layer 2 through output layer\n",
        "        return x\n",
        "\n",
        "model = PimaClassifier()  # initialize the model\n",
        "print(model)  # print the model architecture\n"
      ],
      "metadata": {
        "id": "15uDdrccCW5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715cfc21-2fe7-45d9-85bb-9f26c6cad26c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PimaClassifier(\n",
            "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (act2): ReLU()\n",
            "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
            "  (act_output): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Train the model\n",
        "\n"
      ],
      "metadata": {
        "id": "vmComgWEDOKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()  # binary cross-entropy loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "n_epochs = 100  # number of epochs for training\n",
        "batch_size = 10  # batch size for mini-batch gradient descent\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        Xbatch = X[i:i+batch_size]  # select a mini-batch of inputs\n",
        "        y_pred = model(Xbatch)  # make predictions for the mini-batch\n",
        "        ybatch = y[i:i+batch_size]  # select the corresponding outputs for the mini-batch\n",
        "        loss = loss_fn(y_pred, ybatch)  # compute the loss for the mini-batch\n",
        "        optimizer.zero_grad()  # reset the gradients to zero\n",
        "        loss.backward()  # compute gradients\n",
        "        optimizer.step()  # update model parameters using gradients"
      ],
      "metadata": {
        "id": "Blfw75H1DYon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalulate the model"
      ],
      "metadata": {
        "id": "J2zx1OdGDRcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compute accuracy\n",
        "y_pred = model(X)  # make predictions for the entire dataset\n",
        "accuracy = (y_pred.round() == y).float().mean()  # compute accuracy\n",
        "print(f\"Accuracy {accuracy}\")  # print the accuracy of the model\n",
        "\n",
        "# make class predictions with the model\n",
        "predictions = (model(X) > 0.5).int()  # threshold predicted probabilities at 0.5 to make class predictions\n",
        "for i in range(5):\n",
        "    # print the input variables, predicted class, and actual class for the first 5 examples in the dataset\n",
        "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
      ],
      "metadata": {
        "id": "sAQJrNk0B7C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remark on activation funtions\n",
        "\n",
        "Activation functions are an essential component of neural networks. They introduce non-linearity to the network, allowing it to learn complex patterns and make more accurate predictions. Activation functions are applied to the output of each neuron or layer in a neural network.\n",
        "\n",
        "Here are some commonly used activation functions:\n",
        "\n",
        "* Sigmoid Function: The sigmoid function squashes the input value between 0 and 1.\n",
        "Formula: σ(x) = 1 / (1 + exp(-x))\n",
        "Range: (0, 1)\n",
        "Example: Logistic regression, binary classification problems\n",
        "ReLU (Rectified Linear Unit):\n",
        "\n",
        "* The ReLU function returns the input value if it is positive, and 0 otherwise.\n",
        "Formula: f(x) = max(0, x)\n",
        "Range: [0, +∞)\n",
        "Example: Convolutional Neural Networks (CNNs), deep learning models\n",
        "Leaky ReLU:\n",
        "\n",
        "* The Leaky ReLU function is an extension of ReLU that allows small negative values.\n",
        "Formula: f(x) = max(αx, x), where α is a small positive constant (e.g., 0.01)\n",
        "Range: (-∞, +∞)\n",
        "Example: Neural networks where preventing dead neurons is important\n",
        "Tanh (Hyperbolic Tangent):\n",
        "\n",
        "*  The tanh function maps the input to the range (-1, 1), similar to the sigmoid function but with a steeper gradient.\n",
        "Formula: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "Range: (-1, 1)\n",
        "Example: Recurrent Neural Networks (RNNs)\n",
        "Softmax:\n",
        "\n",
        "* The softmax function is used in multi-class classification problems to convert a vector of real numbers into a probability distribution over classes.\n",
        "Formula: σ(z)_i = exp(z_i) / sum(exp(z_j)) for each element z_i in the input vector z\n",
        "Range: [0, 1] (normalized probabilities that sum to 1)\n",
        "Example: Multi-class classification, output layer of a neural network\n",
        "\n",
        "\n",
        "These are just a few examples of activation functions. Each activation function has different properties and is suitable for different types of problems and network architectures. The choice of activation function depends on the specific requirements and characteristics of the problem at hand."
      ],
      "metadata": {
        "id": "KZ6wcOfOgMn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "input_tensor = torch.randn(10)\n",
        "\n",
        "# Sigmoid Function\n",
        "sigmoid = nn.Sigmoid()\n",
        "output_sigmoid = sigmoid(input_tensor)\n",
        "print(\"Sigmoid:\", output_sigmoid)\n",
        "\n",
        "# ReLU (Rectified Linear Unit)\n",
        "relu = nn.ReLU()\n",
        "output_relu = relu(input_tensor)\n",
        "print(\"ReLU:\", output_relu)\n",
        "\n",
        "# Leaky ReLU\n",
        "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "output_leaky_relu = leaky_relu(input_tensor)\n",
        "print(\"Leaky ReLU:\", output_leaky_relu)\n",
        "\n",
        "# Tanh (Hyperbolic Tangent)\n",
        "tanh = nn.Tanh()\n",
        "output_tanh = tanh(input_tensor)\n",
        "print(\"Tanh:\", output_tanh)\n",
        "\n",
        "# Softmax\n",
        "softmax = nn.Softmax(dim=0)\n",
        "output_softmax = softmax(input_tensor)\n",
        "print(\"Softmax:\", output_softmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_6-L8v2whwR",
        "outputId": "acd61e4a-ecf5-457a-dcc7-426ff9ba1c2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: tensor([0.7462, 0.7571, 0.8109, 0.7599, 0.7049, 0.5072, 0.7223, 0.3963, 0.5618,\n",
            "        0.7307])\n",
            "ReLU: tensor([1.0787, 1.1367, 1.4556, 1.1524, 0.8707, 0.0287, 0.9558, 0.0000, 0.2485,\n",
            "        0.9982])\n",
            "Leaky ReLU: tensor([ 1.0787,  1.1367,  1.4556,  1.1524,  0.8707,  0.0287,  0.9558, -0.0042,\n",
            "         0.2485,  0.9982])\n",
            "Tanh: tensor([ 0.7927,  0.8133,  0.8968,  0.8185,  0.7018,  0.0287,  0.7424, -0.3976,\n",
            "         0.2435,  0.7608])\n",
            "Softmax: tensor([0.1216, 0.1289, 0.1773, 0.1309, 0.0988, 0.0426, 0.1076, 0.0272, 0.0530,\n",
            "        0.1122])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs: https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/"
      ],
      "metadata": {
        "id": "dygN-VrVE78o"
      }
    }
  ]
}