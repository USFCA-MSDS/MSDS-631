{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logits and softmax\n",
        "\n",
        "\n",
        "The logits variable contains the raw output scores from the model's final layer before applying any activation function. It represents the unnormalized predictions or confidence levels for each class. \n",
        "\n",
        "To obtain probabilities, we apply the softmax activation function to the logits using torch.softmax. The softmax function normalizes the logits and produces probabilities that sum up to 1. The resulting probabilities are stored in the probabilities variable."
      ],
      "metadata": {
        "id": "bxBy1S6KqiFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kvppnxBqZ0U",
        "outputId": "4db3a6d7-f7e7-4cb4-cdf8-1bf0f70d4e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits:\n",
            "tensor([[-0.4022, -0.3693,  0.2588],\n",
            "        [-0.5446, -0.5214,  0.1516]], grad_fn=<AddmmBackward0>)\n",
            "Probabilities:\n",
            "tensor([[0.2519, 0.2603, 0.4878],\n",
            "        [0.2482, 0.2540, 0.4978]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a small neural network\n",
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)  # Fully connected layer 1\n",
        "        self.fc2 = nn.Linear(5, 3)   # Fully connected layer 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))   # Apply ReLU activation to layer 1\n",
        "        logits = self.fc2(x)          # Output logits from layer 2\n",
        "        return logits\n",
        "\n",
        "# Create an instance of the network\n",
        "model = SmallNet()\n",
        "\n",
        "# Generate a random input tensor of size (batch_size, input_dim)\n",
        "input_tensor = torch.randn(2, 10)\n",
        "\n",
        "# Forward pass through the network\n",
        "logits = model(input_tensor)\n",
        "\n",
        "# Apply softmax to obtain probabilities\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "print(\"Logits:\")\n",
        "print(logits)\n",
        "print(\"Probabilities:\")\n",
        "print(probabilities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "\n",
        "In PyTorch, the torch.nn.CrossEntropyLoss function computes the cross-entropy loss, which is commonly used in classification tasks. It combines the softmax activation and the negative log-likelihood loss into a single function. Here's an explanation of what goes into the loss calculation:\n",
        "\n",
        "Input: The input to the CrossEntropyLoss function consists of two main components: the predicted logits and the target labels.\n",
        "\n",
        "Predicted Logits: These are the raw output scores from the model's final layer before applying the softmax activation. The logits represent the model's predicted scores for each class in a multi-class classification problem.\n",
        "\n",
        "Target Labels: These are the ground truth labels for the corresponding inputs. The target labels are represented as integers, where each integer corresponds to a specific class.\n",
        "\n",
        "Softmax Activation: Before computing the loss, the predicted logits are passed through the softmax activation function. The softmax function normalizes the logits and converts them into probabilities. This allows us to interpret the outputs as the model's predicted class probabilities.\n",
        "\n",
        "Negative Log-Likelihood Loss: The cross-entropy loss is calculated based on the predicted probabilities and the target labels. It measures the dissimilarity between the predicted probabilities and the target labels. The loss value is higher when the predicted probabilities diverge from the target labels.\n",
        "\n",
        "Loss Calculation: The CrossEntropyLoss function in PyTorch combines the softmax activation and negative log-likelihood loss into a single step. It internally applies the softmax activation to the logits and computes the negative log-likelihood loss.\n",
        "\n",
        "The loss value is computed as the average of the per-instance losses. It can be interpreted as the average dissimilarity between the predicted class probabilities and the true class labels over the entire batch of input examples.\n",
        "\n",
        "The CrossEntropyLoss function automatically performs the softmax activation and the negative log-likelihood loss calculation, providing a convenient way to compute the loss for classification tasks.\n",
        "\n",
        "Here's an example of using CrossEntropyLoss in PyTorch:"
      ],
      "metadata": {
        "id": "6ruF5s1Cr9SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example inputs\n",
        "logits = torch.tensor([[1.2, 0.5, -1.0], [0.3, 1.8, -0.5]])\n",
        "labels = torch.tensor([0, 1])  # Corresponding target labels\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute the loss\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wLEQ8SsBsc",
        "outputId": "2c3ed1e0-6d65-46e5-af7f-11a6ba38abd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3774033188819885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13Lgz0Biqaet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see things in more details \n"
      ],
      "metadata": {
        "id": "V8KITzLqsn3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example input and target labels\n",
        "input_logits = torch.tensor([[1.2, -0.5, 0.8, 2.1]])\n",
        "target_labels = torch.tensor([2])  # Target label for the input example \n",
        "\n",
        "# Compute softmax probabilities using torch.nn.functional.softmax\n",
        "probabilities = F.softmax(input_logits, dim=1)\n",
        "print(\"Probabilities (Softmax):\", probabilities)\n",
        "\n",
        "# Compute log probabilities using torch.log\n",
        "log_probabilities = torch.log(probabilities)\n",
        "print(\"Log Probabilities:\", log_probabilities)\n",
        "\n",
        "# Retrieve the log probability for the target label\n",
        "target_log_probability = log_probabilities[0, target_labels]\n",
        "print(\"Target Log Probability:\", target_log_probability)\n",
        "\n",
        "# Compute the negative log-likelihood loss using torch.neg and torch.mean\n",
        "loss = -torch.mean(target_log_probability)\n",
        "print(\"Loss (Negative Log-Likelihood):\", loss.item())\n",
        "\n",
        "# Compute the loss using torch.nn.CrossEntropyLoss directly\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss_direct = criterion(input_logits, target_labels)\n",
        "print(\"Loss (torch.nn.CrossEntropyLoss):\", loss_direct.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsiKhtQUsnJ8",
        "outputId": "d2262add-6927-4639-8171-12ab6645904f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities (Softmax): tensor([[0.2319, 0.0424, 0.1554, 0.5703]])\n",
            "Log Probabilities: tensor([[-1.4615, -3.1615, -1.8615, -0.5615]])\n",
            "Target Log Probability: tensor([-1.8615])\n",
            "Loss (Negative Log-Likelihood): 1.8615424633026123\n",
            "Loss (torch.nn.CrossEntropyLoss): 1.8615424633026123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using NLL \n",
        "Yet another way to compute the same thing:\n"
      ],
      "metadata": {
        "id": "MoT9GYU8tihM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example input and target labels\n",
        "input_logits = torch.tensor([[1.2, -0.5, 0.8, 2.1]])\n",
        "target_labels = torch.tensor([2])  # Target label for the input example is \"bird\"\n",
        "\n",
        "# Compute log probabilities using torch.nn.functional.log_softmax\n",
        "log_probabilities = F.log_softmax(input_logits, dim=1)\n",
        "print(\"Log Probabilities (Log Softmax):\", log_probabilities)\n",
        "\n",
        "# Retrieve the log probability for the target label\n",
        "target_log_probability = log_probabilities[0, target_labels]\n",
        "print(\"Target Log Probability:\", target_log_probability)\n",
        "\n",
        "# Compute the negative log-likelihood loss using torch.nn.NLLLoss\n",
        "criterion = nn.NLLLoss()\n",
        "loss = criterion(log_probabilities, target_labels)\n",
        "print(\"Loss (torch.nn.NLLLoss):\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUXFOf_isqPk",
        "outputId": "2d793042-7ccb-4f2f-b256-f330f7b5c52b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Probabilities (Log Softmax): tensor([[-1.4615, -3.1615, -1.8615, -0.5615]])\n",
            "Target Log Probability: tensor([-1.8615])\n",
            "Loss (torch.nn.NLLLoss): 1.8615424633026123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5deoPavtlP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}