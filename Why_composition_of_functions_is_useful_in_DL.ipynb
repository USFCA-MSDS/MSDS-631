{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw90CpMwy0-Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Reason 1: Modularity and Reusability\n",
        "# Function composition allows for modular code organization, making it easier to reuse and combine functions in different contexts. This promotes code reusability, enhances maintainability, and facilitates rapid prototyping and experimentation.\n",
        "def relu(x):\n",
        "   return max(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "   return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def composed_function(x):\n",
        "   return sigmoid(relu(x))\n",
        "\n",
        "# Reason 2: Layered Architectures\n",
        "# Function composition naturally lends itself to building layered architectures, such as neural networks, where each layer performs a specific computation. This allows for the creation of complex models by stacking multiple layers together.\n",
        "def dense_layer(x, weights, bias):\n",
        "   return np.dot(x, weights) + bias\n",
        "\n",
        "def relu(x):\n",
        "   return np.maximum(0, x)\n",
        "\n",
        "def composed_network(x):\n",
        "   hidden_layer = relu(dense_layer(x, weights1, bias1))\n",
        "   output_layer = dense_layer(hidden_layer, weights2, bias2)\n",
        "   return output_layer\n",
        "\n",
        "# Reason 3: Expressiveness and Flexibility\n",
        "# Function composition allows you to express complex operations concisely and flexibly, combining nonlinear activations, matrix operations, and other computations. This enables you to design and implement sophisticated deep learning models with ease.\n",
        "def forward_pass(x, weights, biases):\n",
        "   hidden_layer = relu(np.dot(x, weights[0]) + biases[0])\n",
        "   output_layer = sigmoid(np.dot(hidden_layer, weights[1]) + biases[1])\n",
        "   return output_layer\n",
        "\n",
        "def composed_model(x):\n",
        "   return forward_pass(x, [weights1, weights2], [bias1, bias2])\n",
        "\n",
        "# Reason 4: Gradual Building of Complex Models\n",
        "# Function composition enables you to gradually build complex models by adding or modifying individual components. This flexibility allows for incremental improvements and iterative development of deep learning models.\n",
        "def preprocess_data(data):\n",
        "   # Data preprocessing steps\n",
        "   return processed_data\n",
        "\n",
        "def composed_model(x):\n",
        "   preprocessed_data = preprocess_data(x)\n",
        "   hidden_layer = relu(dense_layer(preprocessed_data, weights1, bias1))\n",
        "   output_layer = dense_layer(hidden_layer, weights2, bias2)\n",
        "   return output_layer\n",
        "\n",
        "# Reason 5: Code Organization and Readability\n",
        "# Function composition improves code organization and enhances readability by breaking down complex tasks into smaller, self-contained functions. This makes it easier to understand, maintain, and collaborate on deep learning projects.\n",
        "def feature_extraction(data):\n",
        "   # Feature extraction steps\n",
        "   return features\n",
        "\n",
        "def classification(features):\n",
        "   # Classification steps\n",
        "   return predictions\n",
        "\n",
        "def composed_pipeline(data):\n",
        "   extracted_features = feature_extraction(data)\n",
        "   predictions = classification(extracted_features)\n",
        "   return predictions\n",
        "\n",
        "# Reason 6: Error Handling and Debugging\n",
        "# Function composition allows for easier error handling and debugging by isolating specific components of the overall computation. This helps in identifying and resolving issues in deep learning models.\n",
        "def preprocess_data(data):\n",
        "   # Data preprocessing steps\n",
        "   return processed_data\n",
        "\n",
        "def composed_model(x):\n",
        "   try:\n",
        "       preprocessed_data = preprocess_data(x)\n",
        "       hidden_layer = relu(dense_layer(preprocessed_data, weights1, bias1))\n",
        "       output_layer = dense_layer(hidden_layer, weights2, bias2)\n",
        "       return output_layer\n",
        "   except Exception as e:\n",
        "       print(\"Error occurred:\", str(e))\n",
        "       return None\n",
        "\n",
        "\n",
        "# Reason 7: Scalability and Flexibility\n",
        "# Function composition supports scalable and flexible deep learning models, where new layers or components can be easily added or modified. This allows for model expansion, adaptation to new tasks, and the incorporation of advanced techniques.\n",
        "def composed_model(x):\n",
        "   hidden_layer = relu(dense_layer(x, weights3, bias3))\n",
        "   output_layer = dense_layer(hidden_layer, weights4, bias4)\n",
        "   return output_layer\n",
        "\n",
        "# Reason 8: Transfer Learning\n",
        "# Function composition allows for transfer learning, where pre-trained models or parts of models can be combined with new layers or components. This facilitates leveraging existing knowledge and accelerating training on new tasks.\n",
        "def pre_trained_model(x):\n",
        "   # Pre-trained model computation\n",
        "   return pre_trained_output\n",
        "\n",
        "def additional_layers(x):\n",
        "   # Additional layers computation\n",
        "   return additional_output\n",
        "\n",
        "def transfer_learning_model(x):\n",
        "   pre_trained_output = pre_trained_model(x)\n",
        "   additional_output = additional_layers(pre_trained_output)\n",
        "   return final_output\n",
        "\n",
        "# Reason 9: Data Augmentation\n",
        "# Function composition supports data augmentation techniques by applying transformations to the input data. This enhances the model's ability to generalize by generating additional training examples.\n",
        "def translate_image(image):\n",
        "   # Translate image using transformation\n",
        "   return translated_image\n",
        "\n",
        "def rotate_image(image):\n",
        "   # Rotate image using transformation\n",
        "   return rotated_image\n",
        "\n",
        "def augment_data(image):\n",
        "   translated_image = translate_image(image)\n",
        "   augmented_image = rotate_image(translated_image)\n",
        "   return augmented_image\n",
        "\n",
        "# Reason 10: Sequence Models\n",
        "# Function composition is useful for sequence models, such as recurrent neural networks (RNNs), where computations depend on previous outputs. This enables modeling sequential data, such as time series or natural language.\n",
        "def rnn_cell(x, prev_output):\n",
        "   # RNN cell computation\n",
        "   return output, current_output\n",
        "\n",
        "def rnn_model(sequence):\n",
        "   prev_output = initial_state\n",
        "   outputs = []\n",
        "   for x in sequence:\n",
        "       output, prev_output = rnn_cell(x, prev_output)\n",
        "       outputs.append(output)\n",
        "   return outputs\n",
        "\n",
        "# Reason 11: Attention Mechanisms\n",
        "# Function composition facilitates attention mechanisms, which dynamically focus on different parts of the input data. This enhances the model's ability to capture relevant information and improve performance in tasks such as machine translation or image captioning.\n",
        "def attention_weights(input):\n",
        "   # Attention weights computation\n",
        "   return weights\n",
        "\n",
        "def apply_attention(input, weights):\n",
        "   # Apply attention to input using weights\n",
        "   return attention_output\n",
        "\n",
        "def attention_model(input):\n",
        "   weights = attention_weights(input)\n",
        "   attention_output = apply_attention(input, weights)\n",
        "   return attention_output       \n"
      ]
    }
  ]
}