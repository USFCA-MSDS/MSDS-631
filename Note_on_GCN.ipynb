{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLamwkMfcV76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why right multiplication in GCN : AXW ?\n",
        "\n"
      ],
      "metadata": {
        "id": "QncMJWCAh8LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In graph neural networks (GNNs), the standard formulation involves using right matrix multiplication, i.e., multiplying the adjacency matrix (A) with the node feature matrix (X) on the right side. This formulation allows the nodes to aggregate information from their neighbors and capture the local structural information in the graph.\n",
        "\n",
        "To understand why this formulation is necessary, let's consider a simple graph with three nodes and their corresponding features:"
      ],
      "metadata": {
        "id": "dKJQFFlJcZHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Node 1: [1, 2]\n",
        "# Node 2: [3, 4]\n",
        "# Node 3: [5, 6]"
      ],
      "metadata": {
        "id": "sgqInh6McfNW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = [[0, 1, 0],\n",
        "     [1, 0, 1],\n",
        "     [0, 1, 0]]"
      ],
      "metadata": {
        "id": "04X1MtNycniN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entry A[i, j] represents the weight of the edge between nodes i and j. In this example, node 1 is connected to node 2, node 2 is connected to both node 1 and node 3, and node 3 is connected to node 2.\n",
        "\n",
        "Let's calculate the result of right matrix multiplication (AX) step by step using PyTorch:"
      ],
      "metadata": {
        "id": "IbnBmCwFcyZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Node feature matrix (X)\n",
        "X = torch.tensor([[1, 2],\n",
        "                  [3, 4],\n",
        "                  [5, 6]])\n",
        "\n",
        "# Adjacency matrix (A)\n",
        "A = torch.tensor([[0, 1, 0],\n",
        "                  [1, 0, 1],\n",
        "                  [0, 1, 0]])\n",
        "\n",
        "# Right matrix multiplication: AX\n",
        "AX = torch.matmul(A, X)\n",
        "print(\"AX:\")\n",
        "print(AX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITmy5Xk8ciTu",
        "outputId": "a6967fa2-8ec3-4fde-8f33-d127658820f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AX:\n",
            "tensor([[3, 4],\n",
            "        [6, 8],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, by performing right matrix multiplication (AX), we effectively aggregate the features of neighboring nodes for each node in the graph, capturing the local information.\n",
        "\n",
        "\n",
        "Let's calculate the result of the GNN update rule (AXW) step by step using PyTorch:"
      ],
      "metadata": {
        "id": "-h6Cn8NXdAQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Node feature matrix (X)\n",
        "X = torch.tensor([[1., 2.],\n",
        "                  [3., 4.],\n",
        "                  [5., 6.]])\n",
        "\n",
        "# Adjacency matrix (A)\n",
        "A = torch.tensor([[0., 1., 0.],\n",
        "                  [1., 0., 1.],\n",
        "                  [0., 1., 0.]])\n",
        "\n",
        "# Weight matrix (W)\n",
        "W = nn.Parameter(torch.randn(2, 2))\n",
        "\n",
        "# GNN update: AXW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "AXW = torch.matmul(torch.matmul(A, X), W)\n",
        "output = torch.sigmoid(AXW)\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk_M3VWDc1af",
        "outputId": "26fce93c-7a64-41dd-b56c-07ab936847c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "tensor([[0.6109, 0.2292],\n",
            "        [0.7114, 0.0812],\n",
            "        [0.6109, 0.2292]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we introduced the weight matrix W as an nn.Parameter with dimensions [2, 2]. We performed the GNN update rule by multiplying A and X using right matrix multiplication (torch.matmul(A, X)), followed by multiplying the result with W (torch.matmul(torch.matmul(A, X), W)). Finally, we applied the sigmoid activation function to the result.\n",
        "\n",
        "The inclusion of the weight matrix W allows the GNN to learn and transform the aggregated features according to the task at hand. The choice of activation function and the specific architecture of the GNN can vary depending on the requirements of the problem. The example provided is a basic illustration to demonstrate the inclusion of the weight matrix W in the GNN formulation."
      ],
      "metadata": {
        "id": "7jmsL1lud_NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What goes wrong if we had AWX\n",
        "\n",
        "In the alternative formulation, AWX, we perform element-wise multiplication between the adjacency matrix A and the node feature matrix X, followed by the matrix multiplication with X. However, this formulation leads to a mismatch in dimensions and fails to capture the desired neighborhood aggregation.\n",
        "\n",
        "\n",
        "\n",
        "Specifically, in the standard formulation, the adjacency matrix A represents the connections between nodes in a graph. Each entry A[i, j] represents the weight of the edge between nodes i and j. The node feature matrix X contains the feature vectors of each node, where each row corresponds to a node and each column corresponds to a feature dimension.\n",
        "\n",
        "By multiplying A and X using right matrix multiplication (AX), we effectively perform a weighted sum of the neighboring nodes' features for each node in the graph. This operation allows the node to aggregate information from its neighbors, capturing the local structural information in the graph. The resulting matrix AX has dimensions (N x F), where N is the number of nodes in the graph and F is the number of features.\n",
        "\n",
        "If we were to use the alternative formulation AWX, the multiplication would be done differently. Here, AW would represent a weighted adjacency matrix, where each entry AW[i, j] would denote the contribution of node j's features to node i. However, the subsequent multiplication with X would cause a mismatch in dimensions.\n"
      ],
      "metadata": {
        "id": "NKdH0oPVeKq-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXgmpscPdHRn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Node feature matrix (X)\n",
        "X = torch.tensor([[1., 2.],\n",
        "                  [3., 4.]])\n",
        "\n",
        "# Adjacency matrix (A)\n",
        "A = torch.tensor([[0., 1.],\n",
        "                  [1., 0.]])\n",
        "\n",
        "# Weight matrix (W)\n",
        "W = torch.tensor([[0.1, 0.2],\n",
        "                  [0.3, 0.4],\n",
        "                  [0.5, 0.6]])\n"
      ],
      "metadata": {
        "id": "bM-n-FvveSVZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(W,X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmzEY0ucgN--",
        "outputId": "f8ddefa0-4ab9-480f-f659-64a47b806d21"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7000, 1.0000],\n",
              "        [1.5000, 2.2000],\n",
              "        [2.3000, 3.4000]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But we cannot do\n",
        "torch.matmul(A,torch.matmul(W,X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "cVWIB0Krg1KY",
        "outputId": "07722b24-70a0-4a12-e7d9-22c208003f0e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f3c8f12a39b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# But we cannot do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 3x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooABsiG_hSov"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the dimensions of A are [2 x 2], the dimensions of W are [3 x 2], and the dimensions of X are [2 x 2]. When we attempt to compute AWX using the incorrect order, a dimension mismatch occurs. The error message indicates that the matrix multiplication between [2 x 2] and [2 x 3] is not valid because the number of columns in the first matrix does not match the number of rows in the second matrix.\n",
        "\n",
        "This demonstrates the importance of following the correct order (AXW) in GNN formulations, where the adjacency matrix A is multiplied with the node feature matrix X first, followed by multiplication with the weight matrix W. The correct formulation ensures that the dimensions align correctly for matrix multiplication and preserves the desired neighborhood aggregation."
      ],
      "metadata": {
        "id": "azJcDsVAh4fy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ow0V8uzmhdST"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TvAj4N8VhayB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}