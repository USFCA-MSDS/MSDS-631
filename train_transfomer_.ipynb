{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B4VSgro2KKY3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "# Function to download and extract IMDB dataset\n",
        "def download_extract_imdb(root=\"./imdb_data\"):\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "\n",
        "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    filename = os.path.join(root, \"aclImdb_v1.tar.gz\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    # Extract the tar.gz file\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall(root)\n",
        "\n",
        "# Check if the dataset is downloaded and extracted\n",
        "if not os.path.exists(\"./imdb_data/aclImdb\"):\n",
        "    download_extract_imdb()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Load data\n",
        "def load_imdb_data(root=\"./imdb_data/aclImdb\"):\n",
        "    data = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        label_dir = os.path.join(root, \"train\", label)\n",
        "        for filename in os.listdir(label_dir):\n",
        "            with open(os.path.join(label_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                review = file.read()\n",
        "                # Tokenize review\n",
        "                tokenized_review = tokenizer(review)\n",
        "                data.append((tokenized_review, 1 if label == \"pos\" else 0))\n",
        "    return data\n",
        "\n",
        "# Load training data\n",
        "train_data = load_imdb_data()\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(data):\n",
        "    vocab = set()\n",
        "    for tokens, _ in data:\n",
        "        vocab.update(tokens)\n",
        "    vocab = list(vocab)\n",
        "    vocab.insert(0, '<unk>')  # unknown token\n",
        "    vocab.insert(1, '<pad>')  # padding token\n",
        "    vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab_to_idx\n",
        "\n",
        "vocab_to_idx = build_vocab(train_data)\n",
        "\n",
        "# Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "        pooled_output = torch.mean(transformer_output, dim=0)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "VOCAB_SIZE = len(vocab_to_idx)\n",
        "EMBED_DIM = 60\n",
        "NUM_HEADS = 2\n",
        "HIDDEN_DIM = 60\n",
        "NUM_LAYERS = 1\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "model = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS, NUM_CLASSES).to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# Define the collate function to pad sequences\n",
        "def collate_batch(batch):\n",
        "    text, labels = zip(*batch)\n",
        "    labels = torch.tensor(labels)\n",
        "    # Find the maximum length of text in the batch\n",
        "    max_length = max(len(item) for item in text)\n",
        "    # Create a tensor to hold the padded sequences\n",
        "    padded_text = torch.zeros((len(text), max_length), dtype=torch.long)\n",
        "    for i, item in enumerate(text):\n",
        "        # Fill the tensor with the sequences, leaving the remaining space as padding\n",
        "        padded_text[i, :len(item)] = torch.tensor([vocab_to_idx[token] for token in item])\n",
        "    return padded_text, labels\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_data))\n",
        "val_size = len(train_data) - train_size\n",
        "train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 6\n",
        "\n",
        "# Create data loaders\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Training loop\n",
        "N_EPOCHS = 5\n",
        "\n",
        "\n",
        "# Define the number of epochs\n",
        "N_EPOCHS = 5\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    # Training Phase\n",
        "    print(\"epoch\", epoch)\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_train_loss = 0  # Initialize epoch training loss\n",
        "\n",
        "    for text, labels in train_iterator:\n",
        "        text, labels = text.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(text)\n",
        "\n",
        "        # Compute the loss\n",
        "        train_loss = criterion(predictions, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        train_loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the training loss for this batch\n",
        "        epoch_train_loss += train_loss.item()\n",
        "\n",
        "    # Compute average training loss for the epoch\n",
        "    average_train_loss = epoch_train_loss / len(train_iterator)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    epoch_val_loss = 0  # Initialize epoch validation loss\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during validation\n",
        "        for text, labels in val_iterator:\n",
        "            text, labels = text.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(text)\n",
        "\n",
        "            # Compute the loss\n",
        "            val_loss = criterion(predictions, labels)\n",
        "\n",
        "            # Accumulate the validation loss for this batch\n",
        "            epoch_val_loss += val_loss.item()\n",
        "\n",
        "    # Compute average validation loss for the epoch\n",
        "    average_val_loss = epoch_val_loss / len(val_iterator)\n",
        "\n",
        "    # Print epoch information\n",
        "    print(f'Epoch: {epoch+1:02} | Train Loss: {average_train_loss:.3f} | Val. Loss: {average_val_loss:.3f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3kv1psmSD6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}