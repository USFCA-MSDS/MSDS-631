{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A minimal neural network "
      ],
      "metadata": {
        "id": "pG4W5BmARGu-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUsUS3MgOyr5",
        "outputId": "e674962a-c700-4721-f105-21763977ac0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Mean Squared Error = 0.2298\n",
            "Epoch 100: Mean Squared Error = 0.1689\n",
            "Epoch 200: Mean Squared Error = 0.1677\n",
            "Epoch 300: Mean Squared Error = 0.1672\n",
            "Epoch 400: Mean Squared Error = 0.1670\n",
            "Epoch 500: Mean Squared Error = 0.1670\n",
            "Epoch 600: Mean Squared Error = 0.1669\n",
            "Epoch 700: Mean Squared Error = 0.1669\n",
            "Epoch 800: Mean Squared Error = 0.1669\n",
            "Epoch 900: Mean Squared Error = 0.1668\n",
            "Epoch 1000: Mean Squared Error = 0.1668\n",
            "Epoch 1100: Mean Squared Error = 0.1668\n",
            "Epoch 1200: Mean Squared Error = 0.1668\n",
            "Epoch 1300: Mean Squared Error = 0.1668\n",
            "Epoch 1400: Mean Squared Error = 0.1667\n",
            "Epoch 1500: Mean Squared Error = 0.1668\n",
            "Epoch 1600: Mean Squared Error = 0.1667\n",
            "Epoch 1700: Mean Squared Error = 0.1667\n",
            "Epoch 1800: Mean Squared Error = 0.1667\n",
            "Epoch 1900: Mean Squared Error = 0.1667\n",
            "[[0.5       ]\n",
            " [0.5       ]\n",
            " [0.98531316]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.98531316]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FeedforwardNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Calculate the output of the network given an input X\n",
        "        self.hidden_layer_output = np.maximum(0, np.dot(X, self.weights1))\n",
        "        self.output_layer_output = np.dot(self.hidden_layer_output, self.weights2)\n",
        "        self.output_layer_activation = self.sigmoid(self.output_layer_output)\n",
        "        return self.output_layer_activation\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Sigmoid activation function\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # Derivative of the sigmoid function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def fit(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Backpropagation\n",
        "            error = y - output\n",
        "            output_delta = error * self.sigmoid_derivative(output)\n",
        "            hidden_delta = np.dot(output_delta, self.weights2.T) * (self.hidden_layer_output > 0)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights2 += learning_rate * np.dot(self.hidden_layer_output.T, output_delta)\n",
        "            self.weights1 += learning_rate * np.dot(X.T, hidden_delta)\n",
        "\n",
        "            # Print the mean squared error every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                mse = np.mean(np.square(error))\n",
        "                print(f\"Epoch {epoch}: Mean Squared Error = {mse:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0, 1], [1, 0]])\n",
        "y = np.array([[0], [1], [1], [0], [1], [1]])\n",
        "\n",
        "# Create a feedforward network with 2 input units, 2 hidden units, and 1 output unit\n",
        "network = FeedforwardNetwork(input_size=2, hidden_size=2, output_size=1)\n",
        "\n",
        "# Train the network\n",
        "network.fit(X, y, learning_rate=0.1, epochs=2000)\n",
        "\n",
        "# Make predictions\n",
        "predictions = network.forward(X)\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ejXx5c1iRF7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remark\n",
        "\n",
        "In a feedforward neural network, the weight update process in the backpropagation algorithm involves adjusting the weights to minimize the error between the predicted output and the target output. The weight update equations are as follows:\n",
        "\n",
        "\n",
        "# Update the weights connecting the hidden layer to the output layer\n",
        "weights2 = weights2 + learning_rate * hidden_layer_output.T.dot(output_delta)\n",
        "\n",
        "# Update the weights connecting the input layer to the hidden layer\n",
        "weights1 = weights1 + learning_rate * input_data.T.dot(hidden_delta)\n",
        "\n",
        "\n",
        "Here, `weights2` represents the connections between the hidden layer and the output layer, and `weights1` represents the connections between the input layer and the hidden layer. The learning_rate determines the step size of the weight update.\n",
        "\n",
        "To update `weights2`, we calculate the change or delta in the weights based on the error in the output layer and the output of the hidden layer. This change is then added to the existing `weights2`.\n",
        "\n",
        "To update `weights1`, we calculate the change or delta in the weights based on the error in the hidden layer, the input data, and the output delta from the output layer. This change is then added to the existing `weights1`.\n",
        "\n",
        "The dot product (`dot()`) between matrices is used to perform the matrix multiplication necessary for weight updates. This allows the neural network to adjust its weights in the direction that reduces the error and improves its performance over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "RjLnmRdTQ5Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_KxjF46O4nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}