{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVMR0tp28lSE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Homework Assignment: Comprehensive Exploration of CNN Techniques\n",
        "\n",
        "#### Objective:\n",
        "The objective of this assignment is to explore various techniques used in convolutional neural networks (CNNs) for image classification tasks. Specifically, students will experiment with different regularization techniques and initialization methods to understand their impact on model performance.\n",
        "\n",
        "#### Tasks:\n",
        "1. **Dataset Preparation:**\n",
        "   - Download the CIFAR-10 dataset, a widely used benchmark dataset for image classification.\n",
        "   - Preprocess the dataset by normalizing the pixel values and splitting it into training and testing sets.\n",
        "\n",
        "2. **Experiment 1: Regularization Techniques:**\n",
        "   - Implement a CNN model architecture for image classification using PyTorch.\n",
        "   - Experiment with different regularization techniques:\n",
        "     - No regularization\n",
        "     - L2 regularization\n",
        "     - Dropout regularization\n",
        "   - Train each model using the training set and evaluate its performance on the testing set.\n",
        "   - Compare and analyze the impact of each regularization technique on model performance.\n",
        "\n",
        "3. **Experiment 2: Initialization Techniques:**\n",
        "   - Implement a CNN model architecture for image classification using PyTorch.\n",
        "   - Experiment with different weight initialization techniques:\n",
        "     - Default initialization\n",
        "     - Xavier initialization\n",
        "     - Kaiming initialization\n",
        "   - Train each model using the training set and evaluate its performance on the testing set.\n",
        "   - Compare and analyze the impact of each initialization technique on model performance.\n",
        "\n",
        "4. **Experiment 3: Learning Rate Scheduling:**\n",
        "   - Experiment with different techniques:\n",
        "     - Step decay\n",
        "     - Exponential decay\n",
        "     - Cyclic learning rates\n",
        "   - Train each model using the training set and evaluate its performance on the testing set.\n",
        "   - Compare and analyze the impact of each initialization technique on model performance.\n",
        "\n",
        "\n",
        "5. **Analysis and Conclusion:**\n",
        "   - Analyze the results obtained from the experiments conducted in Steps 2 and 3,4.\n",
        "   - Discuss the strengths and weaknesses of each regularization technique and initialization method.\n",
        "   - Provide insights into how these techniques affect model performance, training convergence, and generalization ability.\n",
        "   - Propose recommendations for selecting appropriate techniques based on the characteristics of the dataset and task.\n",
        "\n",
        "#### Submission Guidelines:\n",
        "- Students are required to submit a Jupyter Notebook containing the implementation of the CNN models with various techniques, along with necessary explanations, comments, and visualizations.\n",
        "- Additionally, students must provide a detailed report summarizing their findings, including comparisons of model performance, analysis of techniques, and insights gained from the experimentation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gnxEx6fS8mSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a CNN on the CIFAR dataset"
      ],
      "metadata": {
        "id": "Vau08HHfC2uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 1\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout regularization\n",
        "\n",
        "        # Weight initialization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Weight decay regularization\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpGv0fVB8nIB",
        "outputId": "23f66eda-70c8-48c5-8960-99147179fac1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/1], Step [100/782], Loss: 1.7917\n",
            "Epoch [1/1], Step [200/782], Loss: 1.3838\n",
            "Epoch [1/1], Step [300/782], Loss: 1.2567\n",
            "Epoch [1/1], Step [400/782], Loss: 1.3939\n",
            "Epoch [1/1], Step [500/782], Loss: 1.3481\n",
            "Epoch [1/1], Step [600/782], Loss: 1.1661\n",
            "Epoch [1/1], Step [700/782], Loss: 1.2332\n",
            "Accuracy of the model on the test images: 62.84 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00u2BHwJ8qhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example for using various regulization techniques"
      ],
      "metadata": {
        "id": "bo4f1f8qBTqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 1\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Experiment 1: Regularization Techniques\n",
        "regularization_methods = {\n",
        "    \"No Regularization\": None,\n",
        "    \"L2 Regularization\": 1e-4\n",
        "    # Add more regularization techniques as needed\n",
        "}\n",
        "\n",
        "print(\"Experiment 1: Regularization Techniques\")\n",
        "for name, regularization in regularization_methods.items():\n",
        "    print(f\"Experimenting with {name}\")\n",
        "    model = CNN().to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if regularization is not None:\n",
        "        # Apply regularization to the optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                       .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pZQ9mEBBfmV",
        "outputId": "6555d5fd-e222-4403-adb4-e7ce6b0c0aee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Experiment 1: Regularization Techniques\n",
            "Experimenting with No Regularization\n",
            "Epoch [1/1], Step [100/782], Loss: 1.5685\n",
            "Epoch [1/1], Step [200/782], Loss: 1.4143\n",
            "Epoch [1/1], Step [300/782], Loss: 1.2986\n",
            "Epoch [1/1], Step [400/782], Loss: 1.3840\n",
            "Epoch [1/1], Step [500/782], Loss: 1.4242\n",
            "Epoch [1/1], Step [600/782], Loss: 1.2984\n",
            "Epoch [1/1], Step [700/782], Loss: 1.0885\n",
            "Accuracy of the model on the test images: 61.03%\n",
            "\n",
            "Experimenting with L2 Regularization\n",
            "Epoch [1/1], Step [100/782], Loss: 1.7951\n",
            "Epoch [1/1], Step [200/782], Loss: 1.3884\n",
            "Epoch [1/1], Step [300/782], Loss: 1.3318\n",
            "Epoch [1/1], Step [400/782], Loss: 1.2071\n",
            "Epoch [1/1], Step [500/782], Loss: 1.6516\n",
            "Epoch [1/1], Step [600/782], Loss: 1.2574\n",
            "Epoch [1/1], Step [700/782], Loss: 1.3983\n",
            "Accuracy of the model on the test images: 61.92%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGuc0zByBgDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some functions you need to learn, read about and experiment with :"
      ],
      "metadata": {
        "id": "ci1ROW_CDgDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default Initialization\n",
        "nn.init.normal_(self.conv1.weight)\n",
        "nn.init.normal_(self.conv2.weight)\n",
        "nn.init.normal_(self.fc1.weight)\n",
        "nn.init.normal_(self.fc2.weight)\n",
        "\n",
        "# Xavier Initialization\n",
        "nn.init.xavier_normal_(self.conv1.weight)\n",
        "nn.init.xavier_normal_(self.conv2.weight)\n",
        "nn.init.xavier_normal_(self.fc1.weight)\n",
        "nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "# Kaiming Initialization\n",
        "nn.init.kaiming_normal_(self.conv1.weight)\n",
        "nn.init.kaiming_normal_(self.conv2.weight)\n",
        "nn.init.kaiming_normal_(self.fc1.weight)\n",
        "nn.init.kaiming_normal_(self.fc2.weight)\n"
      ],
      "metadata": {
        "id": "GimWhT06DjOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam Optimizer\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# SGD Optimizer\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# RMSprop Optimizer\n",
        "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "_lz7ezmJDkdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1122c_mSE1ae"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjUr5ZD8E54k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcW2qqmfE7B8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
