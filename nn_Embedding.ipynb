{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GXQuCbOLVdF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Embedding\n",
        "\n",
        "nn.Embedding is commonly used to create word embeddings, which are dense vector representations of words in a given vocabulary. Here's how nn.Embedding can be used in an NLP context:\n",
        "\n",
        "Define the vocabulary size and embedding dimension: Determine the size of your vocabulary, i.e., the total number of unique words in your dataset, and choose the dimensionality of the word embeddings.\n",
        "\n",
        "Create an instance of nn.Embedding: Initialize an instance of nn.Embedding with the vocabulary size and embedding dimension as parameters.\n",
        "\n",
        "Convert words to indices: Convert your text data into a sequence of word indices. This can be done using various tokenization methods, such as splitting the text into individual words and mapping each word to its corresponding index in the vocabulary.\n",
        "\n",
        "Input the indices to nn.Embedding: Pass the word indices as input to the nn.Embedding layer. It will perform an embedding lookup and return the corresponding word embeddings for each index."
      ],
      "metadata": {
        "id": "m0lBIdECLYa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define vocabulary size and embedding dimension\n",
        "vocab_size = 10000\n",
        "embedding_dim = 300\n",
        "\n",
        "# Create an instance of nn.Embedding\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Convert text data to word indices\n",
        "text = \"I love natural language processing\"\n",
        "word_indices = [10, 25, 7, 62]\n",
        "\n",
        "# Input the word indices to nn.Embedding\n",
        "embedded = embedding(torch.tensor(word_indices))\n",
        "\n",
        "# Output tensor of embedded representations\n",
        "print(embedded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyESTaS6LbFz",
        "outputId": "571e852e-8551-41df-eba2-7ca4362baf9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0738, -0.1783,  0.2075,  ...,  0.3010, -1.5607,  1.2023],\n",
            "        [-1.4363,  1.7356,  0.2727,  ..., -0.4494,  2.1348, -0.5669],\n",
            "        [-0.0817, -0.8147, -0.4312,  ...,  0.9177, -1.0034, -0.2917],\n",
            "        [-0.2999,  2.2093, -0.3547,  ...,  0.7514,  0.0067,  1.3746]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remark \n",
        "\n",
        "nn.Embedding layer in PyTorch is often used to create word embeddings, which are dense vector representations of words in a given vocabulary. Word embeddings capture semantic and syntactic relationships between words, allowing machine learning models to better understand and reason about natural language.\n",
        "\n",
        "The nn.Embedding layer can be seen as a lookup table that maps each word index to its corresponding dense vector representation (word embedding). During the training process, the word embeddings are learned and updated based on the model's objective.\n"
      ],
      "metadata": {
        "id": "sA8E0JLyMIKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intuition and differnce between this layer and word2vec\n",
        "\n",
        "The intuition behind the nn.Embedding layer lies in its ability to map discrete elements, such as words or categorical variables, into dense, continuous vectors in a lower-dimensional space. This mapping is known as word embedding or feature embedding.\n",
        "\n",
        "Compared to other standard word embeddings like one-hot encodings, which represent words as sparse binary vectors, nn.Embedding provides a more compact and meaningful representation of words. It captures the semantic and syntactic relationships between words by placing similar words closer together in the embedding space.\n",
        "\n",
        "The key difference between nn.Embedding and other standard word embedding methods, such as Word2Vec or GloVe, is that nn.Embedding learns the embeddings from scratch during the training of a neural network. It does not rely on pre-existing pre-trained word vectors. Instead, it uses backpropagation to update the embeddings based on the specific task at hand. This allows the model to learn task-specific representations of the words.\n",
        "\n",
        "In contrast, pre-trained word embeddings like Word2Vec or GloVe are pre-computed on large corpora using unsupervised methods. These pre-trained embeddings capture general semantic relationships between words, which can be transferable across different tasks. They are especially useful when you have limited training data or when you want to leverage knowledge learned from a larger dataset.\n",
        "\n",
        "So, the nn.Embedding layer gives you the flexibility to learn word embeddings specific to your task, while pre-trained word embeddings offer a more general representation learned from a large corpus. The choice depends on the availability of data, the size of your training set, and the nature of your NLP task."
      ],
      "metadata": {
        "id": "JEcU9DNhMxWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpxsLVASLfm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}