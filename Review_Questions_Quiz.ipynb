{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJpwff58LLC2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **CNNs (Convolutional Neural Networks):**\n",
        "   Which of the following layers is commonly used in CNNs for reducing spatial dimensions and extracting important features?\n",
        "   - A) Fully Connected Layer\n",
        "   - B) Max Pooling Layer\n",
        "   - C) Recurrent Layer\n",
        "   - D) Softmax Layer\n",
        "   - **Answer: B) Max Pooling Layer**\n",
        "\n",
        "2. **Transformers:**\n",
        "   What is the primary self-attention mechanism used in Transformers for capturing dependencies between input tokens?\n",
        "   - A) LSTM (Long Short-Term Memory)\n",
        "   - B) GRU (Gated Recurrent Unit)\n",
        "   - C) Sigmoid Activation\n",
        "   - D) Multi-head Attention\n",
        "   - **Answer: D) Multi-head Attention**\n",
        "\n",
        "3. **Dense Networks:**\n",
        "   In a fully connected layer of a dense network, each neuron is connected to:\n",
        "   - A) All neurons in the previous layer\n",
        "   - B) Neurons within a certain receptive field\n",
        "   - C) Neurons with the highest activation values\n",
        "   - D) Neurons with the lowest activation values\n",
        "   - **Answer: A) All neurons in the previous layer**\n",
        "\n",
        "4. **Gradient Descent:**\n",
        "   What is the purpose of the learning rate in gradient descent optimization?\n",
        "   - A) To determine the number of epochs\n",
        "   - B) To control the step size during parameter updates\n",
        "   - C) To calculate the loss function\n",
        "   - D) To initialize the model parameters\n",
        "   - **Answer: B) To control the step size during parameter updates**\n",
        "\n",
        "5. **Backpropagation:**\n",
        "   What does backpropagation involve in neural network training?\n",
        "   - A) Adjusting the learning rate\n",
        "   - B) Forward pass through the network\n",
        "   - C) Calculating gradients and updating weights\n",
        "   - D) Applying activation functions\n",
        "   - **Answer: C) Calculating gradients and updating weights**\n",
        "\n",
        "6. **Computation Graph:**\n",
        "   Which of the following best describes a computation graph in neural networks?\n",
        "   - A) A graphical representation of data samples\n",
        "   - B) A directed acyclic graph representing mathematical operations\n",
        "   - C) A graph connecting neurons in the brain\n",
        "   - D) A visualization tool for model architecture\n",
        "   - **Answer: B) A directed acyclic graph representing mathematical operations**\n",
        "\n",
        "7. **CNNs:**\n",
        "   What is the purpose of the convolution operation in convolutional neural networks?\n",
        "   - A) To reduce the dimensionality of the input data\n",
        "   - B) To perform element-wise multiplication\n",
        "   - C) To extract features from input data\n",
        "   - D) To apply non-linear activation functions\n",
        "   - **Answer: C) To extract features from input data**\n",
        "\n",
        "8. **Transformers:**\n",
        "   What advantage do Transformers have over traditional recurrent neural networks (RNNs) in handling long-range dependencies?\n",
        "   - A) They have fewer parameters\n",
        "   - B) They use convolutional layers\n",
        "   - C) They employ self-attention mechanisms\n",
        "   - D) They have simpler architectures\n",
        "   - **Answer: C) They employ self-attention mechanisms**\n",
        "\n",
        "9. **Dense Networks:**\n",
        "   Which activation function is commonly used in dense layers to introduce non-linearity?\n",
        "   - A) ReLU (Rectified Linear Activation)\n",
        "   - B) Sigmoid\n",
        "   - C) Tanh (Hyperbolic Tangent)\n",
        "   - D) Softmax\n",
        "   - **Answer: A) ReLU (Rectified Linear Activation)**\n",
        "\n",
        "10. **Gradient Descent:**\n",
        "    Which variant of gradient descent adjusts the learning rate adaptively for each parameter?\n",
        "    - A) Stochastic Gradient Descent (SGD)\n",
        "    - B) Mini-batch Gradient Descent\n",
        "    - C) AdaGrad (Adaptive Gradient Algorithm)\n",
        "    - D) RMSprop (Root Mean Square Propagation)\n",
        "    - **Answer: D) RMSprop (Root Mean Square Propagation)**\n",
        "\n",
        "11. **Backpropagation:**\n",
        "    What is the purpose of the chain rule in backpropagation?\n",
        "    - A) To compute the Jacobian matrix\n",
        "    - B) To propagate errors from the output to the input layer\n",
        "    - C) To update model parameters simultaneously\n",
        "    - D) To initialize the weights of the network\n",
        "    - **Answer: B) To propagate errors from the output to the input layer**\n",
        "\n",
        "12. **Computation Graph:**\n",
        "    What advantage does a computation graph offer in deep learning frameworks?\n",
        "    - A) It visualizes the training process\n",
        "    - B) It allows for distributed training\n",
        "    - C) It facilitates automatic differentiation\n",
        "    - D) It reduces model complexity\n",
        "    - **Answer: C) It facilitates automatic differentiation**\n",
        "\n",
        "13. **CNNs:**\n",
        "    How does a stride affect the output size in a convolutional layer?\n",
        "    - A) It increases the number of filters\n",
        "    - B) It reduces the receptive field\n",
        "    - C) It decreases the output size\n",
        "    - D) It has no effect on the output size\n",
        "    - **Answer: C) It decreases the output size**\n",
        "\n",
        "14. **Transformers:**\n",
        "    What is the purpose of positional encoding in Transformer models?\n",
        "    - A) To indicate the position of tokens in the sequence\n",
        "    - B) To increase the number of parameters\n",
        "    - C) To perform feature extraction\n",
        "    - D) To apply dropout regularization\n",
        "    - **Answer: A) To indicate the position of tokens in the sequence**\n",
        "\n",
        "15. **Dense Networks:**\n",
        "    Which of the following techniques can help prevent overfitting in dense networks?\n",
        "    - A) Increasing the number of layers\n",
        "    - B) Using a larger learning rate\n",
        "    - C) Adding dropout layers\n",
        "    - D) Decreasing the batch size\n",
        "    - **Answer: C) Adding dropout layers**\n",
        "\n",
        "16. **Gradient Descent:**\n",
        "    What is the disadvantage of using a high learning rate in gradient descent optimization?\n",
        "    - A) Slower convergence\n",
        "    - B) Increased risk of overshooting the minimum\n",
        "    - C) More stable training process\n",
        "    - D) Higher computational cost\n",
        "    - **Answer: B) Increased risk of overshooting the minimum**\n",
        "\n",
        "17. **Backpropagation:**\n",
        "    In backpropagation, what is the purpose of the activation function?\n",
        "    - A) To compute the loss function\n",
        "    - B) To determine the learning rate\n",
        "    - C) To introduce non-linearity\n",
        "    - D) To initialize the weights\n",
        "    - **Answer: C) To introduce non-linearity**\n",
        "\n",
        "18. **Computation Graph:**\n",
        "    How does a computational graph help in debugging neural network models?\n",
        "    - A) It displays real-time training metrics\n",
        "    - B) It visualizes the model architecture\n",
        "    - C) It tracks the flow of gradients during backpropagation\n",
        "    - D) It identifies parameter initialization issues\n",
        "    - **Answer: C) It tracks the flow of gradients during backpropagation**\n",
        "\n",
        "19. **CNNs:**\n",
        "    Which layer is typically used in CNNs to introduce non-linearities between convolutional layers?\n",
        "    - A) Convolutional Layer\n",
        "    - B) Max Pooling Layer\n",
        "    - C) Fully Connected Layer\n",
        "    - D) Activation Layer\n",
        "    - **Answer: D) Activation Layer**\n",
        "\n",
        "20. **Transformers:**\n",
        "    What is the key advantage\n",
        "\n",
        " of the self-attention mechanism in Transformers for capturing long-range dependencies?\n",
        "    - A) It applies convolutional filters to input sequences\n",
        "    - B) It allows each output token to attend to all input tokens\n",
        "    - C) It utilizes recurrent connections between hidden states\n",
        "    - D) It enforces a fixed-size context window\n",
        "    - **Answer: B) It allows each output token to attend to all input tokens**\n",
        "\n"
      ],
      "metadata": {
        "id": "NZu5MjJFLMkp"
      }
    }
  ]
}