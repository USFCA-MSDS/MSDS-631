{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c63WBZhDkAZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivation: Negative Log-Likelihood to Cross-Entropy\n",
        "\n",
        "### Step 1: Negative Log-Likelihood (NLL)\n",
        "The negative log-likelihood for a single data point \\( (x_i, y_i) \\) is defined as:\n",
        "\n",
        "$ \\text{NLL}(x_i, y_i) = -\\log(P(y_i | x_i, \\theta)) $\n",
        "\n",
        "If we assume a binary classification problem, where \\( y_i \\) is either 0 or 1, and \\( \\hat{y}_i \\) is the predicted probability of \\( y_i = 1 \\), then the likelihood function can be written as:\n",
        "\n",
        "$ P(y_i | x_i, \\theta) = \\hat{y}_i^{y_i} \\cdot (1 - \\hat{y}_i)^{1 - y_i} $\n",
        "\n",
        "So, the negative log-likelihood for a single data point becomes:\n",
        "\n",
        "$ \\text{NLL}(x_i, y_i) = -[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $\n",
        "\n",
        "### Step 2: Average Negative Log-Likelihood\n",
        "Now, if we have a dataset with \\( N \\) data points, we can take the average negative log-likelihood over all data points to get the overall loss function:\n",
        "\n",
        "$ \\text{NLL} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $\n",
        "\n",
        "This is essentially the cross-entropy loss for binary classification problems.\n",
        "\n",
        "### Step 3: Generalization to Multiple Classes\n",
        "For multi-class classification, we extend the binary cross-entropy loss to multiple classes. If \\( y_i \\) is a one-hot encoded vector representing the true class label and \\( \\hat{y}_i \\) is the predicted probability distribution over all classes, then the cross-entropy loss becomes:\n",
        "\n",
        "$ \\text{Cross-Entropy} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(\\hat{y}_{i,k}) $\n",
        "\n",
        "where \\( K \\) is the number of classes, \\( y_{i,k} \\) is the \\( k \\)-th element of \\( y_i \\), and \\( \\hat{y}_{i,k} \\) is the \\( k \\)-th element of \\( \\hat{y}_i \\).\n",
        "\n",
        "### Step 4: Simplification for One-Hot Encoded Labels\n",
        "If we have one-hot encoded labels, \\( y_{i,k} \\) will be 0 for all classes except the true class \\( c \\), where \\( y_{i,c} = 1 \\). Therefore, the cross-entropy loss simplifies to:\n",
        "\n",
        "$ \\text{Cross-Entropy} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{y}_{i,\\text{true}}) $\n",
        "\n",
        "This is the cross-entropy loss commonly used in multi-class classification problems.\n"
      ],
      "metadata": {
        "id": "cZTczJ-bkA_7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNbPbyYUkBui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}