{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_whGYVdEibN"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_values = self.fc3(x)\n",
        "        return q_values\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Define the Q-network and optimizer\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "q_network = QNetwork(state_dim, action_dim)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Convert the state to a PyTorch tensor\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass through the Q-network to get Q-values\n",
        "        q_values = q_network(state_tensor)\n",
        "\n",
        "        # Choose the action with the highest Q-value (exploitation)\n",
        "        action = torch.argmax(q_values).item()\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Convert the next state to a PyTorch tensor\n",
        "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
        "\n",
        "        # Calculate the target Q-value using the Bellman equation\n",
        "        with torch.no_grad():\n",
        "            next_q_values = q_network(next_state_tensor)\n",
        "            target_q_value = reward + 0.99 * torch.max(next_q_values)\n",
        "\n",
        "        # Calculate the loss between predicted and target Q-values\n",
        "        loss = nn.MSELoss()(q_values[action], target_q_value)\n",
        "\n",
        "        # Update the Q-network parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Print the total reward of the episode\n",
        "    print(f\"Episode {episode+1}: Total Reward = {reward}\")\n"
      ]
    }
  ]
}