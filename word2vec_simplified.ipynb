{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An introduction to Word2Vec \n",
        "\n",
        "Word2Vec is a widely used algorithm in natural language processing that is designed to generate word embeddings, which are dense vector representations of words. These embeddings capture the semantic and syntactic relationships between words by learning from large amounts of text data. The key idea behind Word2Vec is that words with similar meanings or contexts should have similar vector representations. This approach enables us to represent words as numerical vectors in a continuous vector space, facilitating various downstream tasks such as word similarity, text classification, and language generation. The training process involves a neural network model that predicts the context words given a target word or vice versa. By iteratively updating the model parameters based on these predictions, Word2Vec learns to generate meaningful and informative word embeddings. The resulting embeddings enable us to perform complex operations on words, such as calculating similarities, finding analogies, or clustering words based on their semantic relationships. Overall, Word2Vec provides a powerful and efficient way to transform words into continuous vector representations, unlocking a wide range of possibilities for understanding and working with textual data."
      ],
      "metadata": {
        "id": "YBS6AlW_Jb_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some basics first\n",
        "The code implements a word_tokenize function that splits a given text into individual words or tokens. It uses a loop to iterate through each character in the text, checks if the character is a whitespace or punctuation mark, and appends the current token to a list if it is not empty. The function returns the list of tokens. A sample text is provided to demonstrate the function's usage and print the resulting tokens."
      ],
      "metadata": {
        "id": "zcff17q9KG7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JINtuvQ8NVDu",
        "outputId": "977b7413-b3c9-425b-a824-651f206d9825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'AI', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language']\n"
          ]
        }
      ],
      "source": [
        "# Lets start implementation from the begining and recall the basics:\n",
        "\n",
        "\n",
        "\n",
        "import string\n",
        "\n",
        "def word_tokenize(text):\n",
        "    tokens = []  # List to store the tokens\n",
        "    current_token = \"\"  # Variable to store the characters of the current token\n",
        "    \n",
        "    # Iterate through each character in the text\n",
        "    for char in text:\n",
        "        if char not in string.whitespace and char not in string.punctuation:\n",
        "            # If the character is not whitespace or punctuation, add it to the current token\n",
        "            current_token += char\n",
        "        elif current_token:\n",
        "            # If the current token is not empty, add it to the list of tokens\n",
        "            tokens.append(current_token)\n",
        "            current_token = \"\"  # Reset the current token\n",
        "    \n",
        "    if current_token:\n",
        "        # Add the last token to the list of tokens if it is not empty\n",
        "        tokens.append(current_token)\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Testing the word_tokenize function\n",
        "text = \"Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, how are you today?\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "# Output: ['Hello', 'how', 'are', 'you', 'today']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTf2wvF8Qt-g",
        "outputId": "c5f87661-d82b-4df0-ec5a-2fc667531887"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'how', 'are', 'you', 'today']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenization and vocabulary building\n",
        "def tokenize_corpus(corpus):\n",
        "    tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "    return tokenized_corpus\n",
        "\n",
        "def build_vocab(tokenized_corpus):\n",
        "    word_counts = Counter()\n",
        "    for sentence in tokenized_corpus:\n",
        "        word_counts.update(sentence)\n",
        "\n",
        "    vocabulary = [word for word, _ in word_counts.most_common()]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    return vocabulary, word2idx, idx2word"
      ],
      "metadata": {
        "id": "CQ9yu5O5T0Qo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I like to eat pizza\",\n",
        "    \"Pizza is delicious\",\n",
        "    \"I enjoy eating burgers\",\n",
        "    \"Burgers are tasty\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized = tokenize_corpus(corpus)\n",
        "print(tokenized)\n",
        "\n",
        "# Build vocabulary\n",
        "vocabulary, word2idx, idx2word = build_vocab(tokenized)\n",
        "print(vocabulary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsZCPMjhT2NE",
        "outputId": "fab857e9-b172-42f7-ca4f-e7bd4cb9daa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'to', 'eat', 'pizza'], ['pizza', 'is', 'delicious'], ['i', 'enjoy', 'eating', 'burgers'], ['burgers', 'are', 'tasty']]\n",
            "['i', 'pizza', 'burgers', 'like', 'to', 'eat', 'is', 'delicious', 'enjoy', 'eating', 'are', 'tasty']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_words(corpus):\n",
        "    unique_words = set()\n",
        "    \n",
        "    for sentence in corpus:\n",
        "        # Tokenize the sentence into words\n",
        "        words = sentence.split()\n",
        "        \n",
        "        # Add the words to the set of unique words\n",
        "        unique_words.update(words)\n",
        "    \n",
        "    return unique_words\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I like to eat pizza\",\n",
        "    \"Pizza is delicious\",\n",
        "    \"I enjoy eating burgers\",\n",
        "    \"Burgers are tasty\"\n",
        "]\n",
        "\n",
        "# Extract the set of unique words from the corpus\n",
        "unique_words = extract_unique_words(corpus)\n",
        "\n",
        "# Print the set of unique words\n",
        "print(unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBQKg-7ST4Rf",
        "outputId": "11e35289-52a4-4816-e49c-ffcde647b128"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'burgers', 'Burgers', 'eat', 'eating', 'like', 'to', 'is', 'pizza', 'are', 'enjoy', 'tasty', 'Pizza', 'delicious', 'I'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Skip-Gram model\n",
        "\n",
        "The Skip-gram model is a type of word2vec model that aims to learn word embeddings by predicting the context words given a target word. In the code, each sentence in the corpus is tokenized, and for each target word, the surrounding context words within a window size are extracted. This approach aligns with the Skip-gram model's objective of capturing the context information for each target word.\n",
        "\n",
        "The following code constructs a small corpus of sentences. Each sentence represents a piece of text. The goal is to extract the words and their corresponding context from each sentence. The context refers to the words surrounding the target word within a specific window size.\n",
        "\n",
        "The code initializes two lists, words and labels, to store the target words and their respective context words. It then iterates over each sentence in the corpus and tokenizes the sentence into individual words. For each word in a sentence, it extracts the target word and its left and right context words based on the window size.\n",
        "\n",
        "The target word is added to the words list, while the context words are added to the labels list as a list of words. Finally, the code creates a DataFrame using the words and labels lists, with the columns \"Word\" and \"Context\" respectively, and prints the resulting DataFrame.\n",
        "\n",
        "The DataFrame represents the extracted words and their corresponding context in a tabular format, where each row contains a target word and its associated context words."
      ],
      "metadata": {
        "id": "uDEZQEZMKcH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Constructing a small corpus\n",
        "# corpus = [\n",
        "#     \"The man walked down the street.\",\n",
        "#     \"The woman ran in the park.\",\n",
        "#     \"The man and woman danced together.\",\n",
        "#     \"A group of men and women gathered for a meeting.\",\n",
        "#     \"The man and woman smiled at each other.\",\n",
        "#     \"Both the man and woman played music.\",\n",
        "#     \"The man and woman enjoyed a picnic in the garden.\",\n",
        "#     \"Men and women worked side by side in the office.\",\n",
        "#     \"The man and woman held hands as they strolled along the beach.\",\n",
        "#     \"Both the man and woman laughed at the funny movie.\",\n",
        "#     \"Monday is the start of the workweek.\",\n",
        "#     \"She has a meeting scheduled for Tuesday.\",\n",
        "#     \"We are planning a trip for Wednesday next week.\",\n",
        "#     \"Thursday is her favorite day of the week.\",\n",
        "#     \"They went to a concert on Friday night.\",\n",
        "#     \"I always enjoy relaxing on Saturdays.\",\n",
        "#     \"Sunday mornings are perfect for a brunch with friends.\",\n",
        "#     \"In January, we usually experience cold temperatures.\",\n",
        "#     \"The festival will take place in February.\",\n",
        "#     \"She is looking forward to her birthday in March.\",\n",
        "#     \"The cherry blossoms bloom beautifully in April.\",\n",
        "#     \"May is a great time to go hiking.\",\n",
        "#     \"The summer heat starts to kick in during June.\",\n",
        "#     \"They have a family reunion planned for July.\",\n",
        "#     \"She loves the colors of the leaves in October.\",\n",
        "#     \"November brings a sense of gratitude and reflection.\",\n",
        "#     \"December is filled with holiday celebrations.\",\n",
        "#     \"He is going on a year-long trip around the world.\",\n",
        "#     \"They meet up every other week for a study group.\",\n",
        "#     \"The deadline for the project is just a few days away.\",\n",
        "#     \"She enjoys spending her weekends at the beach.\"\n",
        "# ]\n",
        "\n",
        "corpus =['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']\n",
        "\n",
        "\n",
        "# Initialize lists to store words and labels\n",
        "words = []\n",
        "labels = []\n",
        "\n",
        "# Window size for context\n",
        "window_size = 2\n",
        "\n",
        "# Iterate over each sentence in the corpus\n",
        "for sentence in corpus:\n",
        "    # Tokenize the sentence into words\n",
        "    tokenized_sentence = sentence.split()\n",
        "    \n",
        "    # Iterate over each word in the sentence\n",
        "    for i in range(len(tokenized_sentence)):\n",
        "        # Get the target word\n",
        "        target_word = tokenized_sentence[i]\n",
        "        \n",
        "        # Get the left and right context words within the window size\n",
        "        context_words = tokenized_sentence[max(0, i - window_size): i] + tokenized_sentence[i+1: i+window_size+1]\n",
        "        \n",
        "        # Add the target word and its context words to the lists\n",
        "        words.append(target_word)\n",
        "        labels.append(context_words)\n",
        "\n",
        "# Create a DataFrame from the words and labels lists\n",
        "df = pd.DataFrame({'Word': words, 'Context': labels})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEgN-Nl0U4Uu",
        "outputId": "6fd27019-4a57-4e6c-f5cf-0260012d1a91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Word                       Context\n",
            "0       king                       [is, a]\n",
            "1         is             [king, a, strong]\n",
            "2          a       [king, is, strong, man]\n",
            "3     strong                  [is, a, man]\n",
            "4        man                   [a, strong]\n",
            "5      queen                       [is, a]\n",
            "6         is              [queen, a, wise]\n",
            "7          a      [queen, is, wise, woman]\n",
            "8       wise                [is, a, woman]\n",
            "9      woman                     [a, wise]\n",
            "10       boy                       [is, a]\n",
            "11        is               [boy, a, young]\n",
            "12         a         [boy, is, young, man]\n",
            "13     young                  [is, a, man]\n",
            "14       man                    [a, young]\n",
            "15      girl                       [is, a]\n",
            "16        is              [girl, a, young]\n",
            "17         a      [girl, is, young, woman]\n",
            "18     young                [is, a, woman]\n",
            "19     woman                    [a, young]\n",
            "20    prince                       [is, a]\n",
            "21        is            [prince, a, young]\n",
            "22         a     [prince, is, young, king]\n",
            "23     young                 [is, a, king]\n",
            "24      king                    [a, young]\n",
            "25  princess                       [is, a]\n",
            "26        is          [princess, a, young]\n",
            "27         a  [princess, is, young, queen]\n",
            "28     young                [is, a, queen]\n",
            "29     queen                    [a, young]\n",
            "30       man                  [is, strong]\n",
            "31        is                 [man, strong]\n",
            "32    strong                     [man, is]\n",
            "33     woman                  [is, pretty]\n",
            "34        is               [woman, pretty]\n",
            "35    pretty                   [woman, is]\n",
            "36    prince                       [is, a]\n",
            "37        is              [prince, a, boy]\n",
            "38         a       [prince, is, boy, will]\n",
            "39       boy             [is, a, will, be]\n",
            "40      will            [a, boy, be, king]\n",
            "41        be             [boy, will, king]\n",
            "42      king                    [will, be]\n",
            "43  princess                       [is, a]\n",
            "44        is           [princess, a, girl]\n",
            "45         a    [princess, is, girl, will]\n",
            "46      girl             [is, a, will, be]\n",
            "47      will          [a, girl, be, queen]\n",
            "48        be           [girl, will, queen]\n",
            "49     queen                    [will, be]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "The code implements the Word2Vec model, which is a popular technique for representing words as numerical vectors. Word2Vec captures the semantic and syntactic relationships between words by learning vector representations from large amounts of text data.\n",
        "\n",
        "The Word2Vec model has two main components: an encoder and a decoder. The encoder takes a word as input and converts it into a lower-dimensional vector representation, often called a word embedding. This embedding represents the meaning or context of the word in a continuous vector space.\n",
        "\n",
        "The encoder is trained to encode words in such a way that words with similar meanings or contexts are closer to each other in the vector space. This allows the model to capture the semantic relationships between words. The decoder, on the other hand, reconstructs the original word from its embedding.\n",
        "\n",
        "In the provided code, a vocabulary is created from a given corpus, which is a collection of text data. The vocabulary consists of all unique words present in the corpus. This vocabulary serves as the basis for training the Word2Vec model.\n",
        "\n",
        "The code also sets the dimensions for the word embeddings. These dimensions determine the size of the vector representation for each word. By specifying a higher-dimensional embedding space, the model can potentially capture more nuanced relationships between words.\n",
        "\n",
        "The code demonstrates the usage of the Word2Vec model by creating an instance of a model trainer, which combines the Word2Vec model with a decoder. It also sets up the necessary components for training the model, such as the loss function and optimizer.\n",
        "\n",
        "Overall, the code provides a framework for training a Word2Vec model to learn meaningful word embeddings from text data. These embeddings can then be used for various natural language processing tasks, such as word similarity, document classification, or language generation."
      ],
      "metadata": {
        "id": "_AWyUl2WIsvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.encoder = nn.Linear(input_dim, embedding_dim)    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "class Word2VecModelTrainer(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim):\n",
        "        super(Word2VecModelTrainer, self).__init__()\n",
        "        self.word2vec = Word2Vec(input_dim, embedding_dim)\n",
        "        self.decoder = nn.Linear(embedding_dim, input_dim)\n",
        "    \n",
        "    def forward(self, x,inference=False):\n",
        "        if not inference:\n",
        "          encoded = self.word2vec(x)\n",
        "          decoded = self.decoder(encoded)\n",
        "          return decoded\n",
        "        else:\n",
        "          return self.word2vec(x)\n",
        "\n",
        "\n",
        "# Create vocabulary from the corpus\n",
        "vocab = list(set([word.lower() for sentence in corpus for word in sentence.split()]))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_dim = len(vocab)\n",
        "\n",
        "embedding_dim = 5  # Dimension of the embedding\n",
        "\n",
        "# Create an instance of the autoencoder\n",
        "model = Word2VecModelTrainer(input_dim, embedding_dim)\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "svaqlcGo53ah"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The details of training Word2Vec Model \n",
        "\n",
        "In the provided training loop, the inputs (x) and targets (y) are chosen in a specific way to train the Word2Vec model effectively.\n",
        "\n",
        "In the Word2Vec model, the objective is to learn word embeddings that capture the context of words. The context of a word is defined by the words that appear around it in a given corpus. To capture this context, the training loop uses a sliding window approach.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "* For each sentence in the corpus, the training loop generates a list of words. These words serve as the input data (x) for the model.\n",
        "\n",
        "* The training loop then creates pairs of target words (y) and context words based on a fixed window size. For example, if the window size is set to 3, each target word will have three context words on the left and three context words on the right. This window slides through the sentence, generating multiple pairs of target-context words.\n",
        "\n",
        "* The pairs of target-context words are used to train the Word2Vec model. The model takes the target word (x) as input and predicts the context word (y). The objective is to minimize the difference between the predicted context word and the actual context word.\n",
        "\n",
        "* By training the model to predict context words given target words, the Word2Vec model learns to capture the relationships and similarities between words. Words that often appear in similar contexts will have similar embeddings, allowing the model to represent their semantic or syntactic relationships.\n",
        "\n",
        "* Choosing x as the target word and y as the context word follows the Skip-gram architecture of Word2Vec. The Skip-gram model aims to maximize the likelihood of predicting the surrounding context words based on the given target word. This approach has been shown to be effective in learning high-quality word embeddings.\n",
        "\n",
        "In summary, by selecting the target words as input (x) and the corresponding context words as targets (y), the training loop sets up the Word2Vec model to learn embeddings that capture the context and relationships between words in a given corpus."
      ],
      "metadata": {
        "id": "R8Vk0leDJEsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "additional_words = ['a', 'an', 'the', 'in']\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "stopwords_set\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(word, vocab):\n",
        "    encoding = torch.zeros(len(vocab))\n",
        "    encoding[vocab.index(word)] = 1\n",
        "    return encoding\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Training loop\n",
        "    for sentence in corpus:\n",
        "        # Tokenize the sentence into words\n",
        "        tokenized_sentence = sentence.lower().split()\n",
        "        tokenized_sentence = [word for word in tokenized_sentence if word not in stopwords_set and word not in additional_words]\n",
        "\n",
        "\n",
        "        # Iterate over each word in the sentence\n",
        "        for i, word in enumerate(tokenized_sentence):\n",
        "            # Skip words not in the vocabulary\n",
        "            if word not in vocab:\n",
        "                continue\n",
        "\n",
        "            # Convert the word to one-hot encoding\n",
        "            x = one_hot_encode(word, vocab)\n",
        "\n",
        "            # Get the context words within the window size\n",
        "            context_words = tokenized_sentence[max(0, i - window_size):i] + tokenized_sentence[i+1:i+window_size+1]\n",
        "\n",
        "            # Iterate over each context word\n",
        "            for context_word in context_words:\n",
        "                # Skip context words not in the vocabulary\n",
        "                if context_word not in vocab:\n",
        "                    continue\n",
        "\n",
        "                # Convert the context word to one-hot encoding\n",
        "                y = one_hot_encode(context_word, vocab)\n",
        "\n",
        "                # Forward pass\n",
        "                output = model(x)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = loss_function(output, y)\n",
        "                loss_= loss.item()\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZmAOLSN7Bsh",
        "outputId": "316c8d15-a595-4945-dd1f-fa3a4cc2162c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 2.9967\n",
            "Epoch 2: Loss = 3.0298\n",
            "Epoch 3: Loss = 3.0270\n",
            "Epoch 4: Loss = 2.9949\n",
            "Epoch 5: Loss = 2.9446\n",
            "Epoch 6: Loss = 2.8866\n",
            "Epoch 7: Loss = 2.8273\n",
            "Epoch 8: Loss = 2.7699\n",
            "Epoch 9: Loss = 2.7155\n",
            "Epoch 10: Loss = 2.6645\n",
            "Epoch 11: Loss = 2.6178\n",
            "Epoch 12: Loss = 2.5762\n",
            "Epoch 13: Loss = 2.5410\n",
            "Epoch 14: Loss = 2.5127\n",
            "Epoch 15: Loss = 2.4915\n",
            "Epoch 16: Loss = 2.4769\n",
            "Epoch 17: Loss = 2.4680\n",
            "Epoch 18: Loss = 2.4637\n",
            "Epoch 19: Loss = 2.4629\n",
            "Epoch 20: Loss = 2.4644\n",
            "Epoch 21: Loss = 2.4674\n",
            "Epoch 22: Loss = 2.4713\n",
            "Epoch 23: Loss = 2.4757\n",
            "Epoch 24: Loss = 2.4802\n",
            "Epoch 25: Loss = 2.4847\n",
            "Epoch 26: Loss = 2.4890\n",
            "Epoch 27: Loss = 2.4931\n",
            "Epoch 28: Loss = 2.4971\n",
            "Epoch 29: Loss = 2.5008\n",
            "Epoch 30: Loss = 2.5043\n",
            "Epoch 31: Loss = 2.5076\n",
            "Epoch 32: Loss = 2.5107\n",
            "Epoch 33: Loss = 2.5134\n",
            "Epoch 34: Loss = 2.5158\n",
            "Epoch 35: Loss = 2.5177\n",
            "Epoch 36: Loss = 2.5192\n",
            "Epoch 37: Loss = 2.5202\n",
            "Epoch 38: Loss = 2.5207\n",
            "Epoch 39: Loss = 2.5206\n",
            "Epoch 40: Loss = 2.5200\n",
            "Epoch 41: Loss = 2.5188\n",
            "Epoch 42: Loss = 2.5170\n",
            "Epoch 43: Loss = 2.5146\n",
            "Epoch 44: Loss = 2.5117\n",
            "Epoch 45: Loss = 2.5082\n",
            "Epoch 46: Loss = 2.5042\n",
            "Epoch 47: Loss = 2.4998\n",
            "Epoch 48: Loss = 2.4950\n",
            "Epoch 49: Loss = 2.4897\n",
            "Epoch 50: Loss = 2.4841\n",
            "Epoch 51: Loss = 2.4781\n",
            "Epoch 52: Loss = 2.4719\n",
            "Epoch 53: Loss = 2.4654\n",
            "Epoch 54: Loss = 2.4588\n",
            "Epoch 55: Loss = 2.4519\n",
            "Epoch 56: Loss = 2.4449\n",
            "Epoch 57: Loss = 2.4378\n",
            "Epoch 58: Loss = 2.4306\n",
            "Epoch 59: Loss = 2.4233\n",
            "Epoch 60: Loss = 2.4161\n",
            "Epoch 61: Loss = 2.4088\n",
            "Epoch 62: Loss = 2.4015\n",
            "Epoch 63: Loss = 2.3943\n",
            "Epoch 64: Loss = 2.3871\n",
            "Epoch 65: Loss = 2.3800\n",
            "Epoch 66: Loss = 2.3730\n",
            "Epoch 67: Loss = 2.3661\n",
            "Epoch 68: Loss = 2.3593\n",
            "Epoch 69: Loss = 2.3526\n",
            "Epoch 70: Loss = 2.3460\n",
            "Epoch 71: Loss = 2.3396\n",
            "Epoch 72: Loss = 2.3333\n",
            "Epoch 73: Loss = 2.3272\n",
            "Epoch 74: Loss = 2.3211\n",
            "Epoch 75: Loss = 2.3153\n",
            "Epoch 76: Loss = 2.3096\n",
            "Epoch 77: Loss = 2.3040\n",
            "Epoch 78: Loss = 2.2986\n",
            "Epoch 79: Loss = 2.2933\n",
            "Epoch 80: Loss = 2.2882\n",
            "Epoch 81: Loss = 2.2833\n",
            "Epoch 82: Loss = 2.2785\n",
            "Epoch 83: Loss = 2.2738\n",
            "Epoch 84: Loss = 2.2693\n",
            "Epoch 85: Loss = 2.2649\n",
            "Epoch 86: Loss = 2.2606\n",
            "Epoch 87: Loss = 2.2565\n",
            "Epoch 88: Loss = 2.2526\n",
            "Epoch 89: Loss = 2.2487\n",
            "Epoch 90: Loss = 2.2450\n",
            "Epoch 91: Loss = 2.2414\n",
            "Epoch 92: Loss = 2.2380\n",
            "Epoch 93: Loss = 2.2346\n",
            "Epoch 94: Loss = 2.2314\n",
            "Epoch 95: Loss = 2.2283\n",
            "Epoch 96: Loss = 2.2252\n",
            "Epoch 97: Loss = 2.2223\n",
            "Epoch 98: Loss = 2.2195\n",
            "Epoch 99: Loss = 2.2168\n",
            "Epoch 100: Loss = 2.2142\n",
            "Epoch 101: Loss = 2.2117\n",
            "Epoch 102: Loss = 2.2092\n",
            "Epoch 103: Loss = 2.2069\n",
            "Epoch 104: Loss = 2.2046\n",
            "Epoch 105: Loss = 2.2024\n",
            "Epoch 106: Loss = 2.2003\n",
            "Epoch 107: Loss = 2.1983\n",
            "Epoch 108: Loss = 2.1963\n",
            "Epoch 109: Loss = 2.1944\n",
            "Epoch 110: Loss = 2.1926\n",
            "Epoch 111: Loss = 2.1908\n",
            "Epoch 112: Loss = 2.1891\n",
            "Epoch 113: Loss = 2.1875\n",
            "Epoch 114: Loss = 2.1859\n",
            "Epoch 115: Loss = 2.1844\n",
            "Epoch 116: Loss = 2.1829\n",
            "Epoch 117: Loss = 2.1815\n",
            "Epoch 118: Loss = 2.1801\n",
            "Epoch 119: Loss = 2.1788\n",
            "Epoch 120: Loss = 2.1775\n",
            "Epoch 121: Loss = 2.1763\n",
            "Epoch 122: Loss = 2.1751\n",
            "Epoch 123: Loss = 2.1739\n",
            "Epoch 124: Loss = 2.1728\n",
            "Epoch 125: Loss = 2.1717\n",
            "Epoch 126: Loss = 2.1706\n",
            "Epoch 127: Loss = 2.1696\n",
            "Epoch 128: Loss = 2.1687\n",
            "Epoch 129: Loss = 2.1677\n",
            "Epoch 130: Loss = 2.1668\n",
            "Epoch 131: Loss = 2.1659\n",
            "Epoch 132: Loss = 2.1650\n",
            "Epoch 133: Loss = 2.1642\n",
            "Epoch 134: Loss = 2.1634\n",
            "Epoch 135: Loss = 2.1626\n",
            "Epoch 136: Loss = 2.1619\n",
            "Epoch 137: Loss = 2.1611\n",
            "Epoch 138: Loss = 2.1604\n",
            "Epoch 139: Loss = 2.1597\n",
            "Epoch 140: Loss = 2.1591\n",
            "Epoch 141: Loss = 2.1584\n",
            "Epoch 142: Loss = 2.1578\n",
            "Epoch 143: Loss = 2.1572\n",
            "Epoch 144: Loss = 2.1566\n",
            "Epoch 145: Loss = 2.1560\n",
            "Epoch 146: Loss = 2.1555\n",
            "Epoch 147: Loss = 2.1549\n",
            "Epoch 148: Loss = 2.1544\n",
            "Epoch 149: Loss = 2.1539\n",
            "Epoch 150: Loss = 2.1534\n",
            "Epoch 151: Loss = 2.1529\n",
            "Epoch 152: Loss = 2.1525\n",
            "Epoch 153: Loss = 2.1520\n",
            "Epoch 154: Loss = 2.1516\n",
            "Epoch 155: Loss = 2.1512\n",
            "Epoch 156: Loss = 2.1507\n",
            "Epoch 157: Loss = 2.1503\n",
            "Epoch 158: Loss = 2.1500\n",
            "Epoch 159: Loss = 2.1496\n",
            "Epoch 160: Loss = 2.1492\n",
            "Epoch 161: Loss = 2.1488\n",
            "Epoch 162: Loss = 2.1485\n",
            "Epoch 163: Loss = 2.1482\n",
            "Epoch 164: Loss = 2.1478\n",
            "Epoch 165: Loss = 2.1475\n",
            "Epoch 166: Loss = 2.1472\n",
            "Epoch 167: Loss = 2.1469\n",
            "Epoch 168: Loss = 2.1466\n",
            "Epoch 169: Loss = 2.1463\n",
            "Epoch 170: Loss = 2.1460\n",
            "Epoch 171: Loss = 2.1458\n",
            "Epoch 172: Loss = 2.1455\n",
            "Epoch 173: Loss = 2.1452\n",
            "Epoch 174: Loss = 2.1450\n",
            "Epoch 175: Loss = 2.1447\n",
            "Epoch 176: Loss = 2.1445\n",
            "Epoch 177: Loss = 2.1443\n",
            "Epoch 178: Loss = 2.1440\n",
            "Epoch 179: Loss = 2.1438\n",
            "Epoch 180: Loss = 2.1436\n",
            "Epoch 181: Loss = 2.1434\n",
            "Epoch 182: Loss = 2.1432\n",
            "Epoch 183: Loss = 2.1430\n",
            "Epoch 184: Loss = 2.1428\n",
            "Epoch 185: Loss = 2.1426\n",
            "Epoch 186: Loss = 2.1424\n",
            "Epoch 187: Loss = 2.1423\n",
            "Epoch 188: Loss = 2.1421\n",
            "Epoch 189: Loss = 2.1419\n",
            "Epoch 190: Loss = 2.1418\n",
            "Epoch 191: Loss = 2.1416\n",
            "Epoch 192: Loss = 2.1414\n",
            "Epoch 193: Loss = 2.1413\n",
            "Epoch 194: Loss = 2.1411\n",
            "Epoch 195: Loss = 2.1410\n",
            "Epoch 196: Loss = 2.1408\n",
            "Epoch 197: Loss = 2.1407\n",
            "Epoch 198: Loss = 2.1406\n",
            "Epoch 199: Loss = 2.1404\n",
            "Epoch 200: Loss = 2.1403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to embed words in 2D\n",
        "def embed_words(model, vocab):\n",
        "    embeddings = []\n",
        "    \n",
        "    # Iterate over each word in the vocabulary\n",
        "    for word in vocab:\n",
        "        # Convert the word to one-hot encoding\n",
        "        x = one_hot_encode(word, vocab)\n",
        "        \n",
        "        # Perform forward pass through the autoencoder\n",
        "        embedding = model.word2vec(x).detach().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    \n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Example usage\n",
        "word_embeddings = embed_words(model, vocab)\n",
        "\n",
        "# Print the word embeddings\n",
        "for word, embedding in zip(vocab, word_embeddings):\n",
        "    print(f\"Word: {word}\\tEmbedding: {embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REIZ6TDy4ImI",
        "outputId": "c130394a-ad58-461d-cab8-e7befc980c3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: strong\tEmbedding: [-2.107132   -0.10360259  0.5162208  -0.02196065 -2.4401894 ]\n",
            "Word: is\tEmbedding: [ 0.48701954  0.18305808 -0.12094049  0.4860332  -0.4623993 ]\n",
            "Word: will\tEmbedding: [ 0.57932985  0.525712   -0.14588885  0.2660238  -0.7010566 ]\n",
            "Word: a\tEmbedding: [ 0.28353202  0.18531516 -0.03012578  0.6643554  -0.5734613 ]\n",
            "Word: be\tEmbedding: [ 0.45541757  0.6261656   0.02650623  0.538318   -0.42449644]\n",
            "Word: queen\tEmbedding: [ 1.0296427   0.5204669  -0.13931148 -0.84136915  1.185693  ]\n",
            "Word: young\tEmbedding: [ 0.28502005  1.7396141  -0.77752984 -0.7706263  -1.7760415 ]\n",
            "Word: boy\tEmbedding: [-0.80216515  0.9185308   1.253925    0.8180847  -1.0439075 ]\n",
            "Word: woman\tEmbedding: [ 1.2655801e+00 -1.3711737e+00  1.2331371e+00  1.0979474e-03\n",
            " -1.5045059e-01]\n",
            "Word: king\tEmbedding: [-1.4153827   1.2239678   0.27378684  0.36503237  0.7413472 ]\n",
            "Word: man\tEmbedding: [-1.0976586  -0.21188176 -0.71776915  2.000411    0.51304114]\n",
            "Word: princess\tEmbedding: [ 1.2944089  -0.32903665  1.2005699   0.10675797 -1.2775128 ]\n",
            "Word: pretty\tEmbedding: [ 2.050566    1.1566278  -2.6567678   0.08815852 -0.42890704]\n",
            "Word: wise\tEmbedding: [ 2.4896603   0.30156338 -1.458413    0.42299938 -1.1566794 ]\n",
            "Word: prince\tEmbedding: [-0.40016904 -0.09169465 -1.1395764   1.9059387  -1.0656004 ]\n",
            "Word: girl\tEmbedding: [ 1.9147583   1.1281513  -0.33027822  0.9024081   0.08881074]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1j5cfPyeJahU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot word embeddings in 2D\n",
        "def plot_word_embeddings(embeddings, vocab, num_words=10):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    \n",
        "    # Randomly select a subset of words from the vocabulary\n",
        "    if num_words> len(vocab):\n",
        "      num_words= len(vocab)\n",
        "    selected_words = random.sample(vocab, num_words)\n",
        "    \n",
        "    # Get the corresponding indices of the selected words in the vocabulary\n",
        "    selected_indices = [vocab.index(word) for word in selected_words]\n",
        "    \n",
        "    # Get the embeddings for the selected words\n",
        "    selected_embeddings = embeddings[selected_indices]\n",
        "    \n",
        "    # Decrease the dot size by modifying the 's' parameter\n",
        "    ax.scatter(selected_embeddings[:, 0], selected_embeddings[:, 1], alpha=0.5, s=10)\n",
        "    \n",
        "    # Annotate each point with the corresponding word\n",
        "    for i, word in enumerate(selected_words):\n",
        "        ax.annotate(word, (selected_embeddings[i, 0], selected_embeddings[i, 1]), fontsize=8)\n",
        "    \n",
        "    plt.xlabel(\"Dimension 1\")\n",
        "    plt.ylabel(\"Dimension 2\")\n",
        "    plt.title(\"Word Embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_word_embeddings(word_embeddings, vocab, num_words=len(vocab))\n",
        "vocab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "uWrtCvcY4aWt",
        "outputId": "b71aaef2-6fbe-4c46-d17e-48567b32f28b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAK9CAYAAADCE2/bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfUUlEQVR4nO3deVwW9f7//+cICCgClshSKhIqigoqSuBumEtpdspsVSv1eEzNtI7aomULn8rSk1rm6Sgtdk7ZYlZq7rmviUumooiagJrChSAoy/z+8Of1jdxAgQuYx/12m9thZt4z85rr6ujTN+95j2GapikAAACgkqvi6AIAAACAskDwBQAAgCUQfAEAAGAJBF8AAABYAsEXAAAAlkDwBQAAgCUQfAEAAGAJBF8AAABYAsEXAAAAlkDwBYAysGrVKhmGoVWrVjm6FLvAwEDdfffdpX6dpKQkGYahuLi4a7YdOHCgAgMDC20zDEMvv/xyqdQGwFoIvgAqjS+//FKGYejbb7+9ZF9YWJgMw9DKlSsv2Ve3bl1FR0eXRYnXFBcXJ8Mwrrhs3LjR0SUCQIXl7OgCAKCktGvXTpK0du1a3XvvvfbtGRkZ2r17t5ydnbVu3Tp17tzZvu/o0aM6evSoHnzwwTKv92omTZqk+vXrX7I9ODjYAdU4VnZ2tpyd+esKwI3jTxIAlUZAQIDq16+vtWvXFtq+YcMGmaapvn37XrLv4vrF0Hy9TNNUTk6O3N3db+g8F/Xo0UMRERElcq6Kzs3NzdElAKgkGOoAoFJp166dtm/fruzsbPu2devWKTQ0VD169NDGjRtVUFBQaJ9hGGrbtq0kKS8vT6+++qpuu+02ubq6KjAwUM8//7zOnTtX6DoXx8f+9NNPioiIkLu7uz788ENJ0u+//64+ffqoevXqql27tp555plLjr9RF8fNTp48WTNmzFBQUJCqVaumO++8U0ePHpVpmnr11Vd16623yt3dXffcc49Onz592XMtWbJE4eHhcnNzU5MmTfTNN99c0iY9PV2jRo1SnTp15OrqquDgYL355puFPsuL7QYOHCgvLy95e3trwIABSk9Pv+x158+fr6ZNm8rNzU1Nmza97BAV6dIxvi+//LIMw9CBAwc0cOBAeXt7y8vLS48//rjOnj1b6Njs7GyNHDlStWrVUo0aNdS7d28dO3bsknOeOXNGo0aNUmBgoFxdXVW7dm117dpVv/zyy2VrAlAx0eMLoFJp166dPv30U23atEmdOnWSdCHcRkdHKzo6WjabTbt371bz5s3t+0JCQnTzzTdLkgYNGqSPP/5Y999/v8aMGaNNmzYpNjZWv/322yXBbN++fXrooYf097//XYMHD1ajRo2UnZ2tO+64Q0eOHNHIkSMVEBCgTz/9VCtWrCjWfdhsNv3xxx+FthmGYa/zorlz5+r8+fMaMWKETp8+rbfeeksPPPCAunTpolWrVmns2LE6cOCApk2bpmeffVazZ88udHxCQoL69eunoUOHasCAAZozZ4769u2rxYsXq2vXrpKks2fPqmPHjjp27Jj+/ve/q27dulq/fr3Gjx+vlJQUTZ06VdKFXu977rlHa9eu1dChQ9W4cWN9++23GjBgwCX3t2TJEt13331q0qSJYmNjderUKT3++OO69dZbi/wZPfDAA6pfv75iY2P1yy+/6KOPPlLt2rX15ptv2tsMHDhQX375pR577DHdfvvt+vnnn3XXXXddcq6hQ4fqq6++0vDhw9WkSROdOnVKa9eu1W+//aaWLVsWuSYA5ZwJAJXIr7/+akoyX331VdM0TTM3N9esXr26+fHHH5umaZq+vr7mjBkzTNM0zYyMDNPJyckcPHiwaZqmGR8fb0oyBw0aVOiczz77rCnJXLFihX1bvXr1TEnm4sWLC7WdOnWqKcn88ssv7duysrLM4OBgU5K5cuXKq9Y/Z84cU9JlF1dXV3u7Q4cOmZJMHx8fMz093b59/PjxpiQzLCzMzM3NtW9/6KGHzKpVq5o5OTmX3MPXX39t32az2Ux/f3+zRYsW9m2vvvqqWb16dXP//v2Fah03bpzp5ORkHjlyxDRN05w/f74pyXzrrbfsbfLy8sz27dubksw5c+bYt4eHh5v+/v6Fal+yZIkpyaxXr16h60gyJ06caF+fOHGiKcl84oknCrW79957zZtvvtm+vm3bNlOSOWrUqELtBg4ceMk5vby8zKeeesoEULkx1AFApdK4cWPdfPPN9rG7O3bsUFZWln3WhujoaK1bt07ShbG/+fn59vG9CxculCSNHj260DnHjBkjSfrxxx8Lba9fv766detWaNvChQvl7++v+++/376tWrVqGjJkSLHuY8aMGVq6dGmhZdGiRZe069u3r7y8vOzrkZGRkqRHH3200ANhkZGROn/+vI4dO1bo+ICAgEIPAnp6eqp///7avn27UlNTJUnz5s1T+/btVbNmTf3xxx/2JSYmRvn5+Vq9erX93p2dnfWPf/zDfj4nJyeNGDGi0DVTUlIUHx+vAQMGFKq9a9euatKkSZE/o6FDhxZab9++vU6dOqWMjAxJ0uLFiyVJw4YNK9Tur/VIkre3tzZt2qTk5OQiXx9AxcNQBwCVimEYio6O1urVq1VQUKB169apdu3a9tkQoqOjNX36dEmyB+CLwffw4cOqUqXKJTMn+Pn5ydvbW4cPHy60/XKzLhw+fFjBwcEyDKPQ9kaNGhXrPtq0aVOkh9vq1q1baP1ikKxTp85lt6elpRXafrlaGzZsKOnCOGI/Pz8lJCRo586d8vHxuWwNJ06ckHTh3v39/eXh4VFo/1/v/eLn2KBBg0vO1ahRoyKPq/3rvdesWVPShXv09PS0f59//Z4uNzPGW2+9pQEDBqhOnTpq1aqVevbsqf79+ysoKKhItQCoGOjxBVDptGvXTjabTbt27bKP770oOjpahw8f1rFjx7R27VoFBARcEm7+GgSvpKRmcLgRTk5Oxdpummaxr1FQUKCuXbte0gN9cbnvvvuKfc6SUJL3+MADDygxMVHTpk1TQECA3n77bYWGhl62lx1AxUWPL4BK58/z+a5bt06jRo2y72vVqpVcXV21atUqbdq0ST179rTvq1evngoKCpSQkKDGjRvbtx8/flzp6emqV6/eNa9dr1497d69W6ZpFgrQ+/btK4E7K3kHDhy4pNb9+/dLkv0NarfddpsyMzMVExNz1XPVq1dPy5cvV2ZmZqFe37/e+8XPMSEh4ZJzlOTndPH7PHToUKHe5QMHDly2vb+/v4YNG6Zhw4bpxIkTatmypV5//XX16NGjxGoC4Fj0+AKodCIiIuTm5qa5c+fq2LFjhXp8XV1d1bJlS82YMUNZWVmF5u+9GIIvzlJw0bvvvitJl50N4K969uyp5ORkffXVV/ZtZ8+e1axZs27klkpNcnJyodkqMjIy9Mknnyg8PFx+fn6SLvSGbtiwQT/99NMlx6enpysvL0/ShXvPy8vTBx98YN+fn5+vadOmFTrG399f4eHh+vjjj2Wz2ezbly5dqj179pTYvV0cf/3+++8X2v7XevLz8wvVIUm1a9dWQEBAiU9DB8Cx6PEFUOlUrVpVrVu31po1a+Tq6qpWrVoV2h8dHa133nlHUuEXV4SFhWnAgAGaNWuW0tPT1bFjR23evFkff/yx+vTpU+iNb1cyePBgTZ8+Xf3799e2bdvk7++vTz/9VNWqVSvWPSxatEh79+69ZHt0dHSJjjtt2LChnnzySW3ZskW+vr6aPXu2jh8/rjlz5tjbPPfcc1qwYIHuvvtuDRw4UK1atVJWVpZ27dqlr776SklJSapVq5Z69eqltm3baty4cUpKSrLPCfzXUClJsbGxuuuuu9SuXTs98cQTOn36tKZNm6bQ0FBlZmaWyL21atVK9913n6ZOnapTp07ZpzO72KN9sZf7zJkzuvXWW3X//fcrLCxMHh4eWrZsmbZs2WL/7wRA5UDwBVAptWvXTmvWrLEPbfiztm3b6p133lGNGjUUFhZWaN9HH32koKAgxcXF6dtvv5Wfn5/Gjx+viRMnFum61apV0/LlyzVixAhNmzZN1apV0yOPPKIePXqoe/fuRa5/woQJl90+Z86cEg2+DRo00LRp0/Tcc89p3759ql+/vr744otCs1VUq1ZNP//8s9544w3NmzdPn3zyiTw9PdWwYUO98sor9gfnqlSpogULFmjUqFH67LPPZBiGevfurXfeeUctWrQodN3u3btr3rx5evHFFzV+/HjddtttmjNnjr777jutWrWqxO7vk08+kZ+fn/773//q22+/VUxMjL744gs1atTI/ka4atWqadiwYVqyZIm++eYbFRQUKDg4WO+//36hGSoAVHyGeT1PAQAAUEHFx8erRYsW+uyzz/TII484uhwAZYgxvgCASuvPr66+aOrUqapSpYo6dOjggIoAOBJDHQAAldZbb72lbdu2qXPnznJ2dtaiRYu0aNEiDRky5JK5jgFUfgx1AABUWkuXLtUrr7yiPXv2KDMzU3Xr1tVjjz2mF154odCb7QBYA8EXAAAAlsAYXwAAAFgCwRcAAACWwACnaygoKFBycrJq1KhR6JWeAAAAKB9M09SZM2cUEBCgKlWu3K9L8L2G5ORknvwFAACoAI4ePapbb731ivsJvtdQo0YNSRc+SE9PTwdXAwAAgL/KyMhQnTp17LntSgi+13BxeIOnpyfBFwAAoBy71rBUHm4DAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAcE2TJ0/WkCFD7Ovp6emqVauWTp06peeee05NmzZV06ZNNWLECJ0/f16SNHDgQE2dOtV+zLPPPquXX35ZkvTyyy+rX79+6tWrl5o0aaIuXbro9OnTkqTc3FwNGzZMDRs21O23364xY8aoU6dOZXWrACoxgi8A4JoGDRqk+fPnKz09XZI0Z84c3XPPPfryyy+1ZcsWbdu2TfHx8Tp48KCmTJlSpHNu2rRJcXFx2rNnj2rXrq0PP/xQkjRr1iwlJCTo119/1Zo1a7Rz587Sui0AFkPwBQBck7e3t+6//37Nnj1bpmnqgw8+0PDhw7Vs2TINHDhQrq6ucnZ21uDBg7V06dIinbN79+66+eabJUlRUVE6ePCgJGn58uV69NFH5eLiIhcXFw0YMKDU7guAtTg7ugAAQPmVYstWWlaualZ30ciRI9W7d281btxYPj4+atGixSXtDcOw/+zs7Kz8/Hz7ek5Ojjw8POzrbm5u9p+dnJyUl5d32Rr+fE4AuBH0+AIALmtNwklNWbpf01ckaMrS/TrpdLOCgoI0ZMgQDR8+XJIUExOjTz75ROfPn1deXp4++ugj3XnnnZKk4OBgbd68WZJ06tQpLVy4sEjX7dKliz7//HPl5uYqNzdXn3zySencIADLIfgCAC6RYsvW9zuSZZpSkI+HTFP6fkey7n94gPLy8nT//fdLkoYMGaKWLVuqZcuWCg8PV2BgoEaNGmXfd/LkSTVu3Fj9+/fX7bffXqRr//3vf1dgYKCaNGmitm3b6rbbbpO3t3cp3SkAKzFM0zQdXUR5lpGRIS8vL9lsNnl6ejq6HAAoE3uSMzR9RYKCfDzkVMVQfoGpxJOZSl/+oRoH1dFLL71Uqtc/c+aMatSoodzcXD3yyCNq1aqVxo4dW6rXBFBxFTWvMcYXAHCJmtVd5OHmrOMZOfL1dNPBpCP6ctIQ1b/VVx++906pXz8mJkbnzp1TTk6O2rVrp5EjR5b6NQFUfvT4XgM9vgCsak3CSX2/I1mZOXnycHNWr7AAtW/g4+iyAOAS9PgCAG5I+wY+Cq7tYZ/Vwd/L3dElAcANIfgCAK7I38udwAug0mBWBwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWUKGC7+rVq9WrVy8FBATIMAzNnz//qu1XrVolwzAuWVJTU8umYAAAAJQbFSr4ZmVlKSwsTDNmzCjWcfv27VNKSop9qV27dilVCAAAgPLK2dEFFEePHj3Uo0ePYh9Xu3ZteXt7l3xBAAAAqDAqVI/v9QoPD5e/v7+6du2qdevWXbXtuXPnlJGRUWgBAABAxVepg6+/v79mzpypr7/+Wl9//bXq1KmjTp066ZdffrniMbGxsfLy8rIvderUKcOKAQAAUFoM0zRNRxdxPQzD0Lfffqs+ffoU67iOHTuqbt26+vTTTy+7/9y5czp37px9PSMjQ3Xq1JHNZpOnp+eNlAwAAIBSkJGRIS8vr2vmtQo1xrcktGnTRmvXrr3ifldXV7m6upZhRQAAACgLlXqow+XEx8fL39/f0WUAAACgjFWoHt/MzEwdOHDAvn7o0CHFx8frpptuUt26dTV+/HgdO3ZMn3zyiSRp6tSpql+/vkJDQ5WTk6OPPvpIK1as0JIlSxx1CwAAAHCQChV8t27dqs6dO9vXR48eLUkaMGCA4uLilJKSoiNHjtj3nz9/XmPGjNGxY8dUrVo1NW/eXMuWLSt0DgAAAFhDhX24rawUdbA0AAAAHKOoec1yY3wBAABgTQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8IVlGIah9PT0S7bPnDlTb7/9dtkXBAAAypSzowsAHG3o0KGOLgEAAJQBenxhOaZpauzYserdu7fOnj2rl19+WaNGjZIkxcXFKSYmRg899JCaNWumiIgIJSYm2o+dOHGigoOD1bp1a7344osKDAx0zE0AAIBiI/jCUs6dO6eHHnpImZmZ+vbbb1WtWrVL2mzZskVvvPGGdu3apZiYGL355puSpB9//FFff/21tm/frs2bN+vYsWNlXT4AALgBBF9Yyl133aXQ0FDNmDFDTk5Ol20TFRWl+vXr238+ePCgJGn58uXq27evatSoIcMw9OSTT5ZZ3QAA4MYRfFHppdiytSc5Q5LUpUsXLV26VBkZGVds7+bmZv/ZyclJeXl5l21nGEbJFgoAAEoVwReV2pqEk5qydL+mr0iQJHXqN0R/+9vfFBMTo1OnThXrXF26dNHXX3+tzMxMmaap2bNnl0bJAACglDCrAyqtFFu2vt+RLNOUgnw8JEmLd6do/ON/V/Xq1dWlSxf99NNPRT7f3XffrU2bNik8PFze3t7q2LGjvL29S6l6AABQ0gzTNE1HF1GeZWRkyMvLSzabTZ6eno4uB8WwJzlD01ckKMjHQ05VDOUXmEo8manhXRqoScD1fZdnzpxRjRo1ZJqmxowZo+zsbH3wwQclXDkAACiOouY1enxRadWs7iIPN2cdz8iRr6ebjmfkyMPNWTWru1z3Ofv376+kpCTl5OQoNDRUM2fOLMGKAQBAaSL4otLy93JXr7AAfb8jWYknM+Xh5qxeYQHy93K/7nN+++23JVghAAAoSwRfVGrtG/gouLaH0rJyVbO6yw2FXgAAULExqwMqPX8vdzUJ8CT0AgAqvbi4OO3du9e+Hh8fr//9738OrKh8IfgCAABUEFeaW/4igu/VEXwBAAAczDAMvfjii2rRooUaNmyouXPnFto3ceJEtW7dWuPHj9eZM2c0ePBgtWnTRs2bN9eQIUN0/vx5ffTRR9q6daueeeYZhYeH65NPPtGECRO0cuVKhYeHa+jQoZo8ebKGDBliP3d6erpq1aql06dPO+K2yxxjfAEAAMoBwzC0fft2JSYmKiIiQm3btlVgYKCkC28S3bJliyRpyJAhat++vf7973/LNE0NHjxY//rXv/Tcc8/ps88+06hRo9SnTx9JUkFBgebPn6/58+dLuhB0GzZsqLfeekve3t6aM2eO7rnnHt10000OuOOyR/AFAAAoBwYNGiRJCgoKUocOHbR69Wp78H3iiSfs7ebPn68NGzbo3XfflSRlZ2fLycmpSNfw9vbW/fffr9mzZ+uZZ57RBx98oC+++KJkb6QcI/gCAACUkp49e2rKlClq1KjRJfuSkpIUFhauDb8dueyxhmHYf/bw8LD/bJqmvv76azVs2PC6aho5cqR69+6txo0by8fHRy1atLiu81REjPEFAAAoJQsXLrxs6M3Ly9PmQ6d0Li9f01ckSJJenjxd0oVAvGbNGrVv3/6y5+zTp4/efPNN+4NuaWlpOnDggCTJ09NTNpvN3vav65IUEhKioKAgDRkyRMOHD7/xm6xACL4AAAA36LvvvlPjxo0VFhamsWPHqlatWkpKSlJgYKDi4+MlSZ06ddLIkSMVFRWlTl1itPy345KkIJ8Lvbn7Umxq2jxMd955p9577z37MIe/mjJlitzd3RUeHq7mzZvrjjvuUFJSkqQL43/feOMNhYeHa+HChbrjjjt07tw5NW/eXEOHDrWfY/DgwcrLy9P9999fap9JecRQBwAAgBtw4sQJPfHEE1q3bp1CQkI0Z84cnTp16rJt9+/fr9WrVyvhZLbe+OJnGYYhpyoXhjQ07vqwxrw3WU0CPAsdY5pmoXUPDw9Nnz79sue/++67dffddxfatn79+kvarVy5UsOGDZOLi0uR77MyoMcXAADgBmzcuFHNmzdXSEiIJGnAgAGqWrXqZds++uijcnFxUc3qLqru6iTTNJVfcCHYVndzUs3qpRtEk5OTFRISol9++UWjRo0q1WuVR/T4AgAAXIcUW7bSsnJ1OutckY+5+JCav5e77mjsq48lJZ7M1HPz4tUrLKDU3zIaEBBQ6AUXVkPwBQAAKKY1CSf1/Y5kZebkyThXS7/E79C+ffvUqFEjffbZZzp//vw1z9Gm/s1ydXbS8C4NVLO6S6mHXhB8AQAAiiXFlq3vdyTLNC88mHY8w1mdB72ku3vfo+ruburatas8PDzk7e19zXMZhi4Z04vSQ/AFAAAohrSsXGXm5CnIx0NOVQz5eroprWErfbdys5oEeGr+/Pn64Ycf5O3tbZ9tQZJWrVpV6DyBgYFKT08v09qtjuALAABQDDWru8jDzVnHM3Lk6+mm4xk52r9ynvr+a7gMFcjT01Nz5851dJm4DIIvAABAMfh7uatXWIC+35GsxJOZ8nBz1uuvTFD7Bj6OLg3XQPAFAAAopvYNfBRc20NpWbk8mFaBEHwBAACug7+XO4G3guEFFgAAALAEgi8AAAAsgeALAAAASyD4AgAAwBIIvgAAALAEgi8AAAAsgeALAAAASyD4AgAAwBIIvgAAALAEgi8AAAAsgeALAAAASyD4AgAAwBIIvgAAALAEgi8AAAAsoUIF39WrV6tXr14KCAiQYRiaP3/+NY9ZtWqVWrZsKVdXVwUHBysuLq7U6wQAAED5U6GCb1ZWlsLCwjRjxowitT906JDuuusude7cWfHx8Ro1apQGDRqkn376qZQrBQAAQHnj7OgCiqNHjx7q0aNHkdvPnDlT9evX1zvvvCNJaty4sdauXaspU6aoW7dupVUmAAAAyqEK1eNbXBs2bFBMTEyhbd26ddOGDRuueMy5c+eUkZFRaAEAAEDFV6mDb2pqqnx9fQtt8/X1VUZGhrKzsy97TGxsrLy8vOxLnTp1yqJUAAAAlLJKHXyvx/jx42Wz2ezL0aNHHV0SAAAASkCFGuNbXH5+fjp+/HihbcePH5enp6fc3d0ve4yrq6tcXV3LojwAAACUoUrd4xsVFaXly5cX2rZ06VJFRUU5qCIAAAA4SoUKvpmZmYqPj1d8fLykC9OVxcfH68iRI5IuDFPo37+/vf3QoUOVmJiof/7zn9q7d6/ef/99ffnll3rmmWccUT4AAAAcqEIF361bt6pFixZq0aKFJGn06NFq0aKFJkyYIElKSUmxh2BJql+/vn788UctXbpUYWFheuedd/TRRx8xlRkAAIAFGaZpmo4uojzLyMiQl5eXbDabPD09HV0OAAAA/qKoea1C9fgCKBuGYSg9Pd3RZQAAUKIIvgAAALAEgi+Ay5o8ebJatGihhg0bau7cufbtP/30k1q2bKnmzZurY8eO2rNnjyTp7rvv1ueff25vt2TJEkVGRpZ53QAAXAnBF8BlGYah7du3a/HixRoxYoSSkpJ04sQJPfzww/r444+1c+dODRkyRPfff79M09TTTz+t6dOn24+fMWOGhg8f7sA7AACgMIIvgMsaNGiQJCkoKEgdOnTQ6tWrtWnTJjVr1kzNmjWTJD3yyCNKTk7WsWPH1LVrV9lsNm3fvl2HDx/W5s2b9cADDzjyFgAAKKRSv7kNQPGk2LKVlpV72X2GYVzz+JEjR2ratGny9fXVE088wVsQAQDlCsEXgCRpTcJJfb8jWZk5eZKklydP15xpbyspKUlr1qzR1KlTVb16de3atUu7d+9W06ZN9b///U+33HKLbrnlFknSY489pkmTJik/P19btmxx5O0AAHAJgi8Apdiy9f2OZJmmFOTjIUnal2JT0+ZhOp+Trffee0+BgYGSpLlz56p///7Ky8tTzZo1NW/ePHtvcLVq1fS3v/1NycnJqlOnjqNuBwCAyyL4AlBaVq4yc/IU5OMhpyqG3l68V4knMzW8y2Q1CSg8EXj37t3VvXv3y54nPz9fa9as0bRp08qibAAAioWH2wCoZnUXebg563hGjvILTB3PyJGHm7NqVncp8jkWLFig2267TVFRUWrfvn0pVgsAwPXhlcXXwCuLYRV/HuPr4easXmEBat/Ax9FlAQBwTUXNawx1ACBJat/AR8G1PZSWlaua1V3k7+Xu6JIAAChRBF8Adv5e7gReAEClxRhfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAAAAWALBFwAAAJZA8AUAAIAlEHwBAABgCQRfAIBDGIah9PR0R5cBwEIIvgAAALAEgi8AwGEmT56sFi1aqGHDhpo7d659+5YtW9SlSxdFRESoRYsWmjdvngOrBFBZODu6AACAdRmGoe3btysxMVERERFq27atvL29NWTIEC1cuFD+/v76448/1LJlS0VHR+uWW25xdMkAKjCCLwCgTKXYspWWlStJGjRokCQpKChIHTp00OrVq1WrVi0lJiaqR48ehY7bt28fwRfADSH4AgDKzJqEk/p+R7Iyc/IkSZsPnVK9evXs+w3DkGmaCg0N1fr16x1VJoBKijG+AIAykWLL1vc7kmWaUpCPhyRp6vuzlGLLVlJSktasWaP27dsrOjpahw4d0rJly+zHxsfH6/z5844qHUAlQY8vAKBMpGXlKjMnT0E+HnKqYkiSzp3PU5e2kco/n6P33ntPgYGBkqQff/xRzz77rMaMGaPc3FzVrVtX8+fPd1zxACoFwzRN09FFlGcZGRny8vKSzWaTp6eno8sBgAorxZatKUv3yzQlX083Hc/IkWFIz3RtKH8vd0eXB6ACK2peY6gDAKBM+Hu5q1dYgAxDSjyZKcOQeoUFEHoBlBmGOgAAykz7Bj4Kru2htKxc1azuQugFUKYIvgCAMuXv5U7gBeAQDHUAAACAJRB8AQAAYAkEXwAAAFgCwRcAAACWQPAFAACAJRB8AQAAYAkEXwBApdSzZ0/t27dPktSpUyf7K48HDhyoqVOnOq4wAA7DPL4AgEpp4cKFji4BQDlDjy8AoMKZNWuWhgwZIknas2ePDMPQkiVLJEmTJk3SpEmTFBgYqPj4eAdWCaC8IfgCACqcmJgYLVu2TJK0dOlSRUVFFVqPiYlxZHk3bMGCBWrcuLGaN2+uf/7zn6pVq5aSkpIuCfMRERFatWqVJCk1NVUPPPCA2rRpo2bNmunFF1+0t0tISNBdd92l1q1bq3nz5po+fbp9n2EYeuONN9SmTRvVr19fc+bMKavbBMocwRcAUGGk2LK1JzlD7jf7S5ISExO1bNkyxcbGasWKFcrMzNSePXvUpk0bB1d6/U6cOKHHH39cX3/9tXbu3Kng4GCdOnXqmscNGDBATz31lDZv3qzt27dr69atmjdvnvLz8/XQQw/pnXfe0ZYtW7Rx40bNmjVLW7ZssR/r6uqqzZs3a9GiRRo5cqTy8vJK8xYBh2GMLwCgQliTcFLf70hWZk6ePNyc1bR1Oy1atEgJCQnq2LGjTNPU119/raioKDk7V8y/3lJs2fp64QqFNGmqJk2aSJKefPJJjRgx4qrHZWVlafny5Tp+/Lh9W2Zmpvbt26d9+/bp119/1YMPPmjfd+bMGe3Zs0etW7eWJD3yyCOSpJCQEDk7Oys1NVW33nprSd8e4HAV808GAIClpNiy9f2OZJmmFOTjoeMZOdItTfV/b76lzp06SpK6dOmiiRMnatSoUY4t9jpdDPa7dibraNpZrUk4qfYNfGQYhr2Ns7Oz8vPz7es5OTmSJNM0JUkbN26Um5tbofP++uuvuummm6463vnPxzg5OdHji0qLoQ4AgHIvLStXmTl58vV0k1MVQ76ebqrVoJWO/X7UPp63a9euOnz4sO644w4HV1t8fw72kZFROnk4QXEL1ynFlq3Zs2fr/PnzkqTg4GBt2rRJkrR582b7dG0eHh7q3Lmz/u///s9+zuTkZP3+++9q1KiRPD09C43dPXDggE6fPl2GdwiUDxUu+M6YMUOBgYFyc3NTZGSkNm/efMW2cXFxMgyj0PLXfwkDAMq/mtVd5OHmrOMZOcovMHU8I0e1fGrpWFqW+vfvL0m68847ZZqmmjVrJklKSkpSeHi4JGnVqlXq06ePpAt/N5S3XuE/B3uvm25WvzFv6Ju3nlGnqDZKSEjQzTffLEl67bXXNGPGDIWFhWn27NkKDQ21n2Pu3Lk6cOCAmjZtqmbNmulvf/ubTp06JWdnZ/3www/65ptv1Lx5c4WGhurJJ59Udna2o24XcBjDvPj7kQrgiy++UP/+/TVz5kxFRkZq6tSpmjdvnvbt26fatWtf0j4uLk5PP/20/V/E0oWnV319fYt8zYyMDHl5eclms8nT07NE7gMAUHx/HePbKyxA7Rv4OLqsEpFiy9aUpftlmpKvp5uOZ+TIMKRnujaUv5e7atWqpa1btyowMNDRpQLlUlHzWoUa4/vuu+9q8ODBevzxxyVJM2fO1I8//qjZs2dr3Lhxlz3GMAz5+fmVZZkAgFLQvoGPgmt7KC0rVzWru8jfy93RJZUYfy939QoL0Pc7kpV4MtMe7CvTPQLlQYUJvufPn9e2bds0fvx4+7YqVaooJiZGGzZsuOJxmZmZqlevngoKCtSyZUu98cYbhX419Ffnzp3TuXPn7OsZGRklcwMAgBvm7+VeacPg1YL9H3/84cDKgMqjwozx/eOPP5Sfn3/JMAVfX1+lpqZe9phGjRpp9uzZ+u677/TZZ5+poKBA0dHR+v333694ndjYWHl5edmXOnXqlOh9AABwJf5e7moS4Flpwz3gaBUm+F6PqKgo9e/fX+Hh4erYsaO++eYb+fj46MMPP7ziMePHj5fNZrMvR48eLcOKAQAAUFoqzFCHWrVqycnJqdDk3JJ0/PjxIo/hdXFxUYsWLXTgwIErtnF1dZWrq+sN1QoAAIDyp8L0+FatWlWtWrXS8uXL7dsKCgq0fPlyRUVFFekc+fn52rVrl/z9/UurTAAAAJRTFabHV5JGjx6tAQMGKCIiQm3atNHUqVOVlZVln+Whf//+uuWWWxQbGytJmjRpkm6//XYFBwcrPT1db7/9tg4fPqxBgwY58jYAAADgABUq+Pbr108nT57UhAkTlJqaqvDwcC1evNj+wNuRI0dUpcr/68ROS0vT4MGDlZqaqpo1a6pVq1Zav369/f3nAAAAsI4K9QILR+AFFgAAAOVbUfNahRnjCwAAANwIgi8AAAAsgeALAAAASyD4AgAAwBIIvgAAALAEgi8AAAAsgeALAAAASyD4AgAAwBIIvgAAALCEYgXf7OxsrV27Vnv27LlkX05Ojj755JMSKwwAAAAoSUUOvvv371fjxo3VoUMHNWvWTB07dlRKSop9v81m0+OPP14qRQIAAAA3qsjBd+zYsWratKlOnDihffv2qUaNGmrbtq2OHDlSmvUBAAAAJaLIwXf9+vWKjY1VrVq1FBwcrO+//17dunVT+/btlZiYWJo1AgAAADesyME3Oztbzs7O9nXDMPTBBx+oV69e6tixo/bv318qBQIAAAAlwfnaTS4ICQnR1q1b1bhx40Lbp0+fLknq3bt3yVYGAAAAlKAi9/jee++9+u9//3vZfdOnT9dDDz0k0zRLrDAAAACgJBkmafWqMjIy5OXlJZvNJk9PT0eXAwAAgL8oal7jBRYAAACwBIIvAAAALIHgCwAAAEsg+AIAAMASCL4AAABwiJ49e2rfvn1ldr0iz+P7ZwkJCVq5cqVOnDihgoKCQvsmTJhQIoUBAACgclu4cGGZXq/YPb7//ve/1bhxY02YMEFfffWVvv32W/syf/78UigRAAAAFc2sWbM0ZMgQSdKePXtkGIaWLFkiSZo0aZImTZqkwMBAxcfHS5Jee+01NW7cWOHh4QoPD9fhw4clSVu2bFGXLl0UERGhFi1aaN68edddU7F7fF977TW9/vrrGjt27HVfFAAAAJVbTEyM/u///k+StHTpUkVFRWnZsmW68847tXTpUr355puaPXu2JCktLU2TJ09WSkqK3N3ddfbsWVWpUkXp6ekaMmSIFi5cKH9/f/3xxx9q2bKloqOjdcsttxS7pmL3+Kalpalv377FvhAAAACsIygoSJKUmJioZcuWKTY2VitWrFBmZqb27NmjNm3a2Nt6enqqQYMGevTRR/Xhhx/q9OnTcnNz0/r165WYmKgePXooPDxcMTExknTd44KL3ePbt29fLVmyREOHDr2uCwIAAKByS7FlKy0rV9EdOmnRokVKSEhQx44dZZqmvv76a0VFRcnZ+f/FUCcnJ23cuFHr16/XqlWrdPvtt+u///2vTNNUaGio1q9fXyJ1FTv4BgcH66WXXtLGjRvVrFkzubi4FNo/cuTIEikMAAAAFc+ahJP6fkeyMnPylHFTiF6LfVNdu3SSJHXp0kUTJ07UqFGjCh1z5swZnTlzRu3bt1f79u3166+/avv27Xrsscd06NAhLVu2zN7bGx8fryZNmqhq1arFrs0wTdMszgH169e/8skMQ4mJicUuojwr6rufAQAArC7Flq0pS/fLNCVfTzclHUvVjEGd9a/3Z2nk0EFasmSJunXrpp07d6pZs2YKDAzU/PnzVatWLd1///3KysqSYRhq0KCBZs+eLS8vL/3yyy969tlnderUKeXm5qpu3bqaP3++3Nzc7Nctal4rdvC1GoIvAABA0exJztD0FQkK8vGQUxVD+QWmEk9maniXBmoSUHo5qqh57YZeYGGapsjNAAAAkKSa1V3k4eas4xk5yi8wdTwjRx5uzqpZ3eXaB5eB6wq+n3zyiZo1ayZ3d3e5u7urefPm+vTTT0u6NgAAAFQg/l7u6hUWIMOQEk9myjCkXmEB8vdyd3Rpkq7j4bZ3331XL730koYPH662bdtKktauXauhQ4fqjz/+0DPPPFPiRQIAAKBiaN/AR8G1PZSWlaua1V3KTeiVrvPhtldeeUX9+/cvtP3jjz/Wyy+/rEOHDpVogY7GGF8AAIDyrdTG+KakpCg6OvqS7dHR0UpJSSnu6QAAAIAyUezgGxwcrC+//PKS7V988YUaNGhQIkUBAAAAJa3YY3xfeeUV9evXT6tXr7aP8V23bp2WL19+2UAMAAAAlAfF7vG97777tGnTJtWqVUvz58+3Tzq8efNm3XvvvaVRIwAAAHDDeIHFNfBwGwAAQPlW1LxWpKEOGRkZ9pNkZGRctS3hEAAAAOVRkYJvzZo1lZKSotq1a8vb21uGYVzSxjRNGYah/Pz8Ei8SAAAAuFFFCr4rVqzQTTfdJElauXJlqRYEAAAAlAbG+F4DY3wBAADKt1J7gcXixYu1du1a+/qMGTMUHh6uhx9+WGlpaddXLQDA8sLDw3XmzBlHlwGgEit28H3uuefsD7jt2rVLo0ePVs+ePXXo0CGNHj26xAsEAJQPjzzyiCIiItS8eXPdddddSk1NLdHzx8fHq0aNGiV6TgD4s2IPdfDw8NDu3bsVGBiol19+Wbt379ZXX32lX375RT179izxPwgdjaEOAHDByZMn5ePjI0n6v//7PyUlJWnmzJkldn7DMJSWliZPT0+NHDlSy5cvV9WqVeXs7Kx169bJzc2txK4FoHIp0enM/qxq1ao6e/asJGnZsmXq37+/JOmmm2665lRnAICK6/PPP9enn36qnJwc5eTkqFatWqVynR07dmj58uX69ddfVaVKFdlsNlWtWrVUrgXAWoodfNu1a6fRo0erbdu22rx5s7744gtJ0v79+3XrrbeWeIEAAMdJsWUrLStX+3du0XvvvacNGzaodu3aWrBggSZMmFCi17goKChIeXl5euKJJ9S5c2fdddddqlKl2CPzAOASxf6TZPr06XJ2dtZXX32lDz74QLfccoskadGiRerevXuJFwgAcIw1CSc1Zel+TV+RoE9W/aoqVd1188036/z58/rwww9L/BqStP7gH/Ly8tLu3bv18MMPa+/evWrevLkOHDhQItcDYG3F7vGtW7eufvjhh0u2T5kypUQKAgA4XootW9/vSJZpSkE+HnILi9ae1T8quEFD1fappZiYGB07dqxEryFJi3enqK53VQXU9NCdd96prl276ueff9aePXsUHBxcErcGwMKKHXwlqaCgQAcOHNCJEydUUFBQaF+HDh1KpDAAgOOkZeUqMydPQT4ecqpiKOCmGuoy7A0N79JATQIuPDjy+uuvl+g1JCkrJ197DyRpwPPPKDc3V/n5+Wrbtq169Ohxw/cEAMUOvhs3btTDDz+sw4cP668TQvDKYgCoHGpWd5GHm7OOZ+TI19NNxzNy5OHmrJrVXUrtGs9+GS/DkNre3lDbtm0rsesAwEXFHuM7dOhQRUREaPfu3Tp9+rTS0tLsy+nTp0ujRgBAGfP3clevsAAZhpR4MlOGIfUKC5C/l3uFugYA/Fmx5/GtXr26duzYYZmxVszjC8DKLs64ULO6S6kF0rK4BoDKrdTm8Y2MjNSBAwcsE3wBwMr8vdxLPYyWxTUAQLqO4DtixAiNGTNGqampatasmVxcCo/3at68eYkVBwAAAJSUYg91uNwk4oZhyDTNSvlwG0MdAAAAyrdSG+pw6NChGyoMAAAAcIRiB9969eqVRh0AAABAqbqul59/+umnatu2rQICAnT48GFJ0tSpU/Xdd9+VaHEAAABASSl28P3ggw80evRo9ezZU+np6fYxvd7e3po6dWpJ1wcAAACUiGIH32nTpunf//63XnjhBTk5Odm3R0REaNeuXSVaHAAAAFBSih18Dx06pBYtWlyy3dXVVVlZWSVSFAAAAFDSih1869evr/j4+Eu2L168WI0bNy6JmgAAAIASV+xZHUaPHq2nnnpKOTk5Mk1Tmzdv1n//+1/Fxsbqo48+Ko0aAQAAgBtW7OA7aNAgubu768UXX9TZs2f18MMPKyAgQP/617/04IMPlkaNAAAAwA0r9pvb/uzs2bPKzMxU7dq1S7KmcoU3twEAAJRvpfbmtj+rVq2aqlWrdiOnAAAAAMpEsYPvqVOnNGHCBK1cuVInTpxQQUFBof2nT58useIAAACAklLs4PvYY4/pwIEDevLJJ+Xr6yvDMEqjLgAAAKBEFTv4rlmzRmvXrlVYWFhp1AMAAACUimLP4xsSEqLs7OzSqAUAAAAoNcUOvu+//75eeOEF/fzzzzp16pQyMjIKLQAAAEB5VOyhDt7e3srIyFCXLl0KbTdNU4ZhKD8/v8SKAwAAAEpKsYPvI488IhcXF33++ec83AYAAIAKo9jBd/fu3dq+fbsaNWpUGvUAAAAApaLYY3wjIiJ09OjR0qgFAAAAKDXF7vEdMWKEnn76aT333HNq1qyZXFxcCu1v3rx5iRUHAAAAlBTDNE2zOAdUqXJpJ7FhGJX24baivvsZAAAAjlHUvFbsoQ6HDh26ZElMTLT/b2mbMWOGAgMD5ebmpsjISG3evPmq7efNm6eQkBC5ubmpWbNmWrhwYanXCAAAgPKn2EMd6tWrVxp1FMkXX3yh0aNHa+bMmYqMjNTUqVPVrVs37du3T7Vr176k/fr16/XQQw8pNjZWd999tz7//HP16dNHv/zyi5o2beqAOwAAAICjFGmow4IFC9SjRw+5uLhowYIFV23bu3fvEivuryIjI9W6dWtNnz5dklRQUKA6depoxIgRGjdu3CXt+/Xrp6ysLP3www/2bbfffrvCw8M1c+bMIl2ToQ4AAADlW1HzWpF6fPv06aPU1FTVrl1bffr0uWK70hzje/78eW3btk3jx4+3b6tSpYpiYmK0YcOGyx6zYcMGjR49utC2bt26af78+Ve8zrlz53Tu3Dn7Om+jAwAAqByKNMa3oKDAPpSgoKDgiktpPtj2xx9/KD8/X76+voW2+/r6KjU19bLHpKamFqu9JMXGxsrLy8u+1KlT58aLBwAAgMMV++G2ym78+PGy2Wz2hTmLAQAAKodiPdxWUFCguLg4ffPNN0pKSpJhGKpfv77uv/9+PfbYY6X6+uJatWrJyclJx48fL7T9+PHj8vPzu+wxfn5+xWovSa6urnJ1db3xggEAAFCuFLnH1zRN9e7dW4MGDdKxY8fUrFkzhYaG6vDhwxo4cKDuvffe0qxTVatWVatWrbR8+XL7toKCAi1fvlxRUVGXPSYqKqpQe0launTpFdsDAACg8ipyj29cXJxWr16t5cuXq3PnzoX2rVixQn369NEnn3yi/v37l3iRF40ePVoDBgxQRESE2rRpo6lTpyorK0uPP/64JKl///665ZZbFBsbK0l6+umn1bFjR73zzju666679L///U9bt27VrFmzSq1GAAAAlE9F7vH973//q+eff/6S0CtJXbp00bhx4zR37twSLe6v+vXrp8mTJ2vChAkKDw9XfHy8Fi9ebH+A7ciRI0pJSbG3j46O1ueff65Zs2YpLCxMX331lebPn88cvgAAABZU5FcW+/n5afHixQoPD7/s/u3bt6tHjx5XnTGhImIeXwAAgPKtxF9ZfPr06UumBvszX19fpaWlFa9KAAAAoIwUOfjm5+fL2fnKQ4KdnJyUl5dXIkUBAAAAJa3ID7eZpqmBAwdecaqvP7/tDAAqkgULFmjlypWaMmWKo0sBAJSiIo/xvThzwrXMmTPnhgoqbxjjC1RueXl5V/1tFgCg/CtqXivyn/aVLdBWRElJSVq8eLGGDh3q6FKAcs8wDL3wwgv68ccflZWVpYkTJ+qRRx6x75swYYIWLlyoTp06KTQ0VPPnz9f8+fO1atUqDR8+XB06dNC6deuUl5enjz/+WBEREZKkH3/8US+//LLOnz8vwzD04YcfKjIyUlu2bNHYsWOVkZGh/Px8Pf/88+rbt68jPwIAwF/QzVGBJCUlaebMmVcMvvRcAYUZhqHt27crMTFRERERatu2rQIDAyVdeC5hy5Ytki7MU/5ne/fu1X/+8x+9//77mjlzpl544QX99NNP2r9/vx5//HGtXr1aISEhys3N1dmzZ5Wenq4hQ4Zo4cKF8vf31x9//KGWLVsqOjpat9xySxnfNQDgSkhJ5VR2drYGDhyoXbt2ycXFRb6+vjpy5IgOHz6s8PBw1a1bVwsWLFBgYKD69eunlStXqkGDBvrwww81cuRIbd68WZLUt29fTZw4UZLUqVMnRUREaNOmTUpOTlbXrl01c+ZMSVJKSooGDBig33//XbfeeqtuuukmhYSE6OWXX3bURwBclxRbttKyciVJgwYNkiQFBQWpQ4cOWr16tT34PvHEE1c8R3BwsCIjIyVdeAPk5MmTJV1482P37t0VEhIiSXJxcZGXl5cWLlyoxMRE9ejRo9B59u3bR/AFgHKE4FtOLV68WOnp6dqzZ4+kC9PJ7dy5U6NGjVJ8fHyhtqdOndKmTZtkGIbGjh2rc+fOaefOncrOzla7du0UEhKifv36SZIOHjyolStXKjc3V02aNNGGDRsUFRWlkSNHKioqSq+88opSU1MVHh5u/8sdqCjWJJzU9zuSlZlzYYaZzYdOqV69evb9hmHYf/bw8Ljiedzc3Ow/F2XGGtM0FRoaqvXr119v6QCAMlDk6cxQtsLCwvTbb79p2LBh+uKLL+Ti4nLFtgMHDrT/hb5s2TINHjxYVapUUfXq1dW/f38tXbrU3rZfv35ydnaWu7u7wsPDdfDgQUnS8uXL7T1gfn5+uvvuu0vx7oCSl2LL1vc7kmWaUpDPhVA79f1ZSrFlKykpSWvWrFH79u1v6BrdunXTTz/9pL1790qScnNzZbPZFB0drUOHDmnZsmX2tvHx8Tp//vwNXQ8AULIIvuVMii1be5Iz5H6zv/bs2aPu3btr3bp1atq06RVfEHK1nqs/93BJRe/J+utxQHmXlpWrzJw8+Xq6yanKhf9+z53PU5e2kbrzzjv13nvv2Yc5XK/g4GDNmTNHjz76qMLCwhQZGal9+/apZs2a+vHHH/XGG28oLCxMTZo00bhx41RQUFACdwYAKCkMdShH/vxrWjPrlO69vaF69+6t7t27a/78+br55ptls9mueo6YmBj95z//UceOHXX27Fl9+umnGjt27DWv3aVLF8XFxWnixIk6fvy4fvjhB/39738vqVsDSl3N6i7ycHPW8Ywc+Xpe+Ade9L0DNL7PNPl7uRdq+9dZHAcOHKiBAwdKujAW/s/DiZo2baqkpCT7es+ePdWzZ89Lrt+yZUutWLGiZG4GAFAq6PEtJ/76a9oThxP06D3dFNq0uVq0aKHHHntM0dHRCg0NVdOmTdW7d+/Lnuell16Si4uLmjVrpsjISPXu3VsPPPDANa//r3/9S2vWrFGTJk30yCOPKDIyUt7e3iV8l0Dp8fdyV6+wABmGlHgyU5LUvan/JaEXAGBdRX6BhVWV1Qss9iRnaPqKBAX5eMipiqH8AlOJJzM1vEsDNQko/RdnZGdny8XFRc7Ozjp16pRuv/12ffbZZ/Yn24GK4uKsDjWruxB6AcAiSvwFFihdf/017fGMHHm4Oatm9Ss/1FaSEhIS1L9/f5mmqfPnz2vYsGGEXlRI/l7uBF4AwGURfMuJi7+m/X5HshJPZsrDzVm9wgLK7C/w5s2bXzJNGgAAQGVC8C1H2jfwUXBtD35NCwAAUAoIvuUMv6YFAAAoHczqAAAAAEsg+AIAAMASCL4AAACwBIIvAAAALIHgCwAAAEsg+AIAAMASCL4AAACwBIIvAAAALIHgCwAAAEsg+ALlgGEYev311xUZGanAwEDNnz9fsbGxioiIUIMGDbRq1SpJUl5enrp166aIiAiFhobq4YcfVlZWliRp1apVatq0qYYNG6awsDCFhoZq69atDrwrAADKF4IvUE54eHho06ZN+s9//qNHH31U/v7+2rp1q9544w0999xzkiQnJyd9/vnn2rp1q3bv3i0vLy9NmzbNfo69e/dqwIAB2rFjh0aMGKEXXnjBUbcDAEC54+zoAgArS7FlKy0rV5LUr18/SVJERISysrL04IMPSpLatGmjhIQESZJpmpoyZYp+/PFH5eXlyWazKTo62n6+4OBgRUZGSpKioqI0efLksrwdAADKNYIv4CBrEk7q+x3JyszJkyT9cixTPf0u9OpKkpubm6QL63l5F9p8/vnnWrFihX7++Wd5enrqvffe04oVK+znvHjMX48DAAAMdQAcIsWWre93JMs0pSAfD0nS4t0pSrFlX/W4tLQ01apVS56enjpz5ozi4uLKoFoAACoHgi/gAGlZucrMyZOvp5ucqhiSpKycfPuwhyvp37+/zp49q0aNGqlHjx5q3759WZQLAEClYJimaTq6iPIsIyNDXl5estls8vT0dHQ5qCRSbNmasnS/TFPy9XTT8YwcGYb0TNeG8vdyd3R5AABUKEXNa/T4Ag7g7+WuXmEBMgwp8WSmDEPqFRZA6AUAoBTxcBvgIO0b+Ci4tofSsnJVs7oLoRcAgFJG8AUcyN/LncALAEAZYagDAAAALIHgCwAAAEsg+AIAAMASCL4AAACwBIIvAAAALIHgCwAAAEsg+AIAAMASCL4AAACwBIIvAAAALIHgCwAArtuCBQv0zDPPOLoMoEgM0zRNRxdRnmVkZMjLy0s2m02enp6OLgcAgHIjLy9Pzs7Oji4DKHJeo8cXAAAUYhiGXnzxRbVo0UINGzbU3LlzC+2bOHGiWrdurfHjxysuLk59+vSRJK1atUpNmzbVsGHDFBYWptDQUG3dutV+7I8//qjWrVsrLCxM4eHh2rRpkyRpy5Yt6tKliyIiItSiRQvNmzdPknTy5EndeeedatasmZo3b67HH39ckrRx40a1atVK4eHhatq0qT744IMy+mRQ0fHPNAAAcAnDMLR9+3YlJiYqIiJCbdu2VWBgoCTJyclJW7ZskSTFxcUVOm7v3r36z3/+o/fff18zZ87UCy+8oJ9++kn79+/X448/rtWrVyskJES5ubk6e/as0tPTNWTIEC1cuFD+/v76448/1LJlS0VHR+vLL79U/fr1tWTJEknS6dOnJUmxsbF69tln9dBDD0mS0tLSyuZDQYVHjy8AAJAkpdiytSc5Q5I0aNAgSVJQUJA6dOig1atX29s98cQTVzxHcHCwIiMjJUlRUVE6ePCgJGnp0qXq3r27QkJCJEkuLi7y8vLS+vXrlZiYqB49eig8PFwxMTGSpH379un222/XokWLNGbMGH333XeqXr26JKlz58569dVXNWnSJK1du1Y1a9Ys4U8ClRU9vgAAQGsSTur7HcnKzMmTJG0+dEr16tWz7zcMw/6zh4fHFc/j5uZm/9nJyUl5eXlXva5pmgoNDdX69esvuz8+Pl7Lli3TN998o5deeknbt2/XqFGjdM8992jZsmV6/vnn1bRpU73//vtFuk9YGz2+AABYXIotW9/vSJZpSkE+F0Lt1PdnKcWWraSkJK1Zs0bt27e/oWt069ZNP/30k/bu3StJys3Nlc1mU3R0tA4dOqRly5bZ28bHx+v8+fM6dOiQPDw89MADD2jatGnav3+/MjMztW/fPtWvX1+DBw/W888/r40bN95QbbAOenwBALC4tKxcZebkKcjHQ05VLvTsnjufpy5tI5V/PkfvvfeefXzv9QoODtacOXP06KOPKjc3V05OTpo5c6batGmjH3/8Uc8++6zGjBmj3Nxc1a1bV/Pnz9eqVav07rvv2nuO3377bXl5eenFF1/UihUrVLVqVTk5Oemdd94pgU8BVsB0ZtfAdGYAgMouxZatKUv3yzQlX083Pdc9RCPiVmt8nwj5e7k7ujzgmpjODAAAFIm/l7t6hQXIMKTEk5mSpO5N/Qm9qHQY6gAAANS+gY+Ca3soLStXL6WfJfSiUiL4AgAASRd6fgm8qMwY6gAAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEipM8D19+rQeeeQReXp6ytvbW08++aQyMzOvekynTp1kGEahZejQoWVUMQAAAMoTZ0cXUFSPPPKIUlJStHTpUuXm5urxxx/XkCFD9Pnnn1/1uMGDB2vSpEn29WrVqpV2qQAAACiHKkTw/e2337R48WJt2bJFERERkqRp06apZ8+emjx5sgICAq54bLVq1eTn51dWpQIAAKCcqhBDHTZs2CBvb2976JWkmJgYValSRZs2bbrqsXPnzlWtWrXUtGlTjR8/XmfPnr1q+3PnzikjI6PQAgAAgIqvQvT4pqamqnbt2oW2OTs766abblJqauoVj3v44YdVr149BQQEaOfOnRo7dqz27dunb7755orHxMbG6pVXXimx2gEAAFA+ODT4jhs3Tm+++eZV2/z222/Xff4hQ4bYf27WrJn8/f11xx136ODBg7rtttsue8z48eM1evRo+3pGRobq1Klz3TUAAACgfHBo8B0zZowGDhx41TZBQUHy8/PTiRMnCm3Py8vT6dOnizV+NzIyUpJ04MCBKwZfV1dXubq6FvmcAAAAqBgcGnx9fHzk4+NzzXZRUVFKT0/Xtm3b1KpVK0nSihUrVFBQYA+zRREfHy9J8vf3v656AQAAUHFViIfbGjdurO7du2vw4MHavHmz1q1bp+HDh+vBBx+0z+hw7NgxhYSEaPPmzZKkgwcP6tVXX9W2bduUlJSkBQsWqH///urQoYOaN2/uyNsBAACAA1SI4CtdmJ0hJCREd9xxh3r27Kl27dpp1qxZ9v25ubnat2+ffdaGqlWratmyZbrzzjsVEhKiMWPG6L777tP333/vqFsAAACAAxmmaZqOLqI8y8jIkJeXl2w2mzw9PR1dDgAAAP6iqHmtwvT4AgAAADeC4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEipM8H399dcVHR2tatWqydvbu0jHmKapCRMmyN/fX+7u7oqJiVFCQkLpFgoAAIByqcIE3/Pnz6tv3776xz/+UeRj3nrrLb333nuaOXOmNm3apOrVq6tbt27KyckpxUoBAABQHhmmaZqOLqI44uLiNGrUKKWnp1+1nWmaCggI0JgxY/Tss89Kkmw2m3x9fRUXF6cHH3ywSNfLyMiQl5eXbDabPD09b7R8AAAAlLCi5rUK0+NbXIcOHVJqaqpiYmLs27y8vBQZGakNGzZc8bhz584pIyOj0AIAAICKr9IG39TUVEmSr69voe2+vr72fZcTGxsrLy8v+1KnTp1SrRMAAABlw6HBd9y4cTIM46rL3r17y7Sm8ePHy2az2ZejR4+W6fUBAABQOpwdefExY8Zo4MCBV20TFBR0Xef28/OTJB0/flz+/v727cePH1d4ePgVj3N1dZWrq+t1XRMAAADll0ODr4+Pj3x8fErl3PXr15efn5+WL19uD7oZGRnatGlTsWaGAAAAQOVQYcb4HjlyRPHx8Tpy5Ijy8/MVHx+v+Ph4ZWZm2tuEhITo22+/lSQZhqFRo0bptdde04IFC7Rr1y71799fAQEB6tOnj4PuAgAAAI7i0B7f4pgwYYI+/vhj+3qLFi0kSStXrlSnTp0kSfv27ZPNZrO3+ec//6msrCwNGTJE6enpateunRYvXiw3N7cyrR0AAACOV+Hm8S1rzOMLAABQvll+Hl8AAADgzwi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAFjdr1iwNGTJEkrRnzx4ZhqElS5ZIkiZNmqRJkyZp69atio6OVvPmzdWmTRutW7dOkpSUlCRvb2+99NJLatmypRo0aKB169bpmWeeUXh4uJo2bardu3dLklJTU9W5c2e1atVKoaGhGj58uAoKCiRJcXFxiomJ0UMPPaRmzZopIiJCiYmJDvg0UJkRfAEAsLiYmBgtW7ZMkrR06VJFRUUVWu/UqZP+9re/aeLEidq5c6feffdd3XfffcrMzJQk2Ww2tWrVSr/88ovGjRunbt26qXfv3oqPj9eAAQP0yiuvSJK8vb31/fffa9u2bdq5c6eSkpL05Zdf2uvYsmWL3njjDe3atUsxMTF68803y/iTQGVH8AUAwOKCgoIkSYmJiVq2bJliY2O1YsUKZWZmas+ePapZs6aqVKmibt26SZLatWsnX19fxcfHS5Lc3NzUp08fSVJERIQ8PDzUuXNnSVKbNm2UkJAgSSooKNDYsWMVFhamFi1aaOvWrfZzSFJUVJTq169v//ngwYNlcPewEmdHFwAAABwnxZattKxcRXfopEWLFikhIUEdO3aUaZr6+uuvFRUVddnjDMOw/+zq6mr/2cnJSW5uboXW8/LyJEnvvvuuTpw4oU2bNsnNzU2jR49WTk6Ove2VjgNKCj2+AABY1JqEk5qydL+mr0hQxk0hei32TbVp00aS1KVLF02cOFExMTFq1KiRCgoKtHTpUknS+vXrlZqaqvDw8GJdLy0tTX5+fnJzc1NqaqrmzZtX0rcEXBXBFwAAC0qxZev7HckyTSnIx0N1m0YqNfl3RUR3kCR17dpVhw8f1h133KGqVavqm2++0cSJE9W8eXONGjVKX331lTw8PIp1zaefflqbNm1SaGioHnvsMcXExJTGrQFXZJimaTq6iPIsIyNDXl5estls8vT0dHQ5AACUiD3JGZq+IkFBPh5yqmIov8BU4slMDe/SQE0C+PsOFUtR8xo9vgAAWFDN6i7ycHPW8Ywc5ReYOp6RIw83Z9Ws7uLo0oBSQ/AFAMCC/L3c1SssQIYhJZ7MlGFIvcIC5O/l7ujSgFLDrA4AAFhU+wY+Cq7tobSsXNWs7kLoRaVH8AUAwML8vdwJvLAMhjoAAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsIQKE3xff/11RUdHq1q1avL29i7SMQMHDpRhGIWW7t27l26hAAAAKJecHV1AUZ0/f159+/ZVVFSU/vOf/xT5uO7du2vOnDn2dVdX19IoDwAAAOVchQm+r7zyiiQpLi6uWMe5urrKz8+vFCoCAABARVJhhjpcr1WrVql27dpq1KiR/vGPf+jUqVNXbX/u3DllZGQUWgAAAFDxVerg2717d33yySdavny53nzzTf3888/q0aOH8vPzr3hMbGysvLy87EudOnXKsGIAAACUFocG33Hjxl3y8Nlfl7179173+R988EH17t1bzZo1U58+ffTDDz9oy5YtWrVq1RWPGT9+vGw2m305evTodV8fAAAA5YdDx/iOGTNGAwcOvGqboKCgErteUFCQatWqpQMHDuiOO+64bBtXV1cegAMAAKiEHBp8fXx85OPjU2bX+/3333Xq1Cn5+/uX2TUBAABQPlSYWR2OHDmi06dP68iRI8rPz1d8fLwkKTg4WB4eHpKkkJAQxcbG6t5771VmZqZeeeUV3XffffLz89PBgwf1z3/+U8HBwerWrVuRr2uapiTxkBsAAEA5dTGnXcxtV2RWEAMGDDAlXbKsXLnS3kaSOWfOHNM0TfPs2bPmnXfeafr4+JguLi5mvXr1zMGDB5upqanFuu7Ro0cve10WFhYWFhYWFpbytRw9evSquc74/wMjrqCgoEDJycmqUaOGDMNwdDmXlZGRoTp16ujo0aPy9PR0dDkoRXzX1sD3bB1819bBd126TNPUmTNnFBAQoCpVrjx3Q4UZ6uAoVapU0a233uroMorE09OT/zNZBN+1NfA9WwfftXXwXZceLy+va7ap1PP4AgAAABcRfAEAAGAJBN9KwNXVVRMnTmT+YQvgu7YGvmfr4Lu2Dr7r8oGH2wAAAGAJ9PgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPhWMklJSXryySdVv359ubu767bbbtPEiRN1/vx5R5eGEvb6668rOjpa1apVk7e3t6PLQQmaMWOGAgMD5ebmpsjISG3evNnRJaGErV69Wr169VJAQIAMw9D8+fMdXRJKQWxsrFq3bq0aNWqodu3a6tOnj/bt2+fosiyN4FvJ7N27VwUFBfrwww/166+/asqUKZo5c6aef/55R5eGEnb+/Hn17dtX//jHPxxdCkrQF198odGjR2vixIn65ZdfFBYWpm7duunEiROOLg0lKCsrS2FhYZoxY4ajS0Ep+vnnn/XUU09p48aNWrp0qXJzc3XnnXcqKyvL0aVZFtOZWcDbb7+tDz74QImJiY4uBaUgLi5Oo0aNUnp6uqNLQQmIjIxU69atNX36dElSQUGB6tSpoxEjRmjcuHEOrg6lwTAMffvtt+rTp4+jS0EpO3nypGrXrq2ff/5ZHTp0cHQ5lkSPrwXYbDbddNNNji4DwDWcP39e27ZtU0xMjH1blSpVFBMTow0bNjiwMgAlwWazSRJ/JzsQwbeSO3DggKZNm6a///3vji4FwDX88ccfys/Pl6+vb6Htvr6+Sk1NdVBVAEpCQUGBRo0apbZt26pp06aOLseyCL4VxLhx42QYxlWXvXv3Fjrm2LFj6t69u/r27avBgwc7qHIUx/V8zwCA8u+pp57S7t279b///c/RpVias6MLQNGMGTNGAwcOvGqboKAg+8/Jycnq3LmzoqOjNWvWrFKuDiWluN8zKpdatWrJyclJx48fL7T9+PHj8vPzc1BVAG7U8OHD9cMPP2j16tW69dZbHV2OpRF8KwgfHx/5+PgUqe2xY8fUuXNntWrVSnPmzFGVKnTsVxTF+Z5R+VStWlWtWrXS8uXL7Q86FRQUaPny5Ro+fLhjiwNQbKZpasSIEfr222+1atUq1a9f39ElWR7Bt5I5duyYOnXqpHr16mny5Mk6efKkfR89RpXLkSNHdPr0aR05ckT5+fmKj4+XJAUHB8vDw8OxxeG6jR49WgMGDFBERITatGmjqVOnKisrS48//rijS0MJyszM1IEDB+zrhw4dUnx8vG666SbVrVvXgZWhJD311FP6/PPP9d1336lGjRr2sfpeXl5yd3d3cHXWxHRmlUxcXNwV/4Lkq65cBg4cqI8//viS7StXrlSnTp3KviCUmOnTp+vtt99WamqqwsPD9d577ykyMtLRZaEErVq1Sp07d75k+4ABAxQXF1f2BaFUGIZx2e1z5sy55rA2lA6CLwAAACyBwZ8AAACwBIIvAAAALIHgCwAAAEsg+AIAAMASCL4AAACwBIIvAAAALIHgCwAAAEsg+AIAAMASCL4A4ACGYWj+/PmOLuOqVq1aJcMwlJ6e7uhSAKBEEHwBoIQMHDhQhmHIMAy5uLjI19dXXbt21ezZs1VQUFCobUpKinr06OGgSosmOjpaKSkp8vLyKtXrrF69Wr169VJAQECF+AcBgIqL4AsAJah79+5KSUlRUlKSFi1apM6dO+vpp5/W3Xffrby8PHs7Pz8/ubq6OrDSa6tatar8/PxkGEapXicrK0thYWGaMWNGqV4HAAi+AFCCXF1d5efnp1tuuUUtW7bU888/r++++06LFi1SXFycvd2fezaTkpJkGIa+/PJLtW/fXu7u7mrdurX279+vLVu2KCIiQh4eHurRo4dOnjxZ6HofffSRGjduLDc3N4WEhOj999+377t43m+++UadO3dWtWrVFBYWpg0bNtjbHD58WL169VLNmjVVvXp1hYaGauHChZIuP9Th66+/VmhoqFxdXRUYGKh33nmnUD2BgYF644039MQTT6hGjRqqW7euZs2addXPrEePHnrttdd07733FuejBoBiI/gCQCnr0qWLwsLC9M0331y13cSJE/Xiiy/ql19+kbOzsx5++GH985//1L/+9S+tWbNGBw4c0IQJE+zt586dqwkTJuj111/Xb7/9pjfeeEMvvfSSPv7440LnfeGFF/Tss88qPj5eDRs21EMPPWTvfX7qqad07tw5rV69Wrt27dKbb74pDw+Py9a3bds2PfDAA3rwwQe1a9cuvfzyy3rppZcKBXpJeueddxQREaHt27dr2LBh+sc//qF9+/ZdxycHACXL2dEFAIAVhISEaOfOnVdt8+yzz6pbt26SpKeffloPPfSQli9frrZt20qSnnzyyUIhc+LEiXrnnXf0t7/9TZJUv3597dmzRx9++KEGDBhQ6Lx33XWXJOmVV15RaGioDhw4oJCQEB05ckT33XefmjVrJkkKCgq6Yn3vvvuu7rjjDr300kuSpIYNG2rPnj16++23NXDgQHu7nj17atiwYZKksWPHasqUKVq5cqUaNWpUlI8KAEoNPb4AUAZM07zmWNnmzZvbf/b19ZUkeyC9uO3EiROSLoyLPXjwoJ588kl5eHjYl9dee00HDx684nn9/f0lyX6ekSNH6rXXXlPbtm01ceLEq4bz3377zR7CL2rbtq0SEhKUn59/2esZhiE/Pz/79QDAkQi+AFAGfvvtN9WvX/+qbVxcXOw/XwzJf912cXaIzMxMSdK///1vxcfH25fdu3dr48aN1zzvxfMMGjRIiYmJeuyxx7Rr1y5FRERo2rRp13ubl1zvr3UDgCMRfAGglK1YsUK7du3SfffdV2Ln9PX1VUBAgBITExUcHFxouVbA/qs6depo6NCh+uabbzRmzBj9+9//vmy7xo0ba926dYW2rVu3Tg0bNpSTk9N13wsAlBXG+AJACTp37pxSU1OVn5+v48ePa/HixYqNjdXdd9+t/v37l+i1XnnlFY0cOVJeXl7q3r27zp07p61btyotLU2jR48u0jlGjRqlHj16qGHDhkpLS9PKlSvVuHHjy7YdM2aMWrdurVdffVX9+vXThg0bNH369EIzSVyPzMxMHThwwL5+6NAhxcfH66abblLdunVv6NwA8GcEXwAoQYsXL5a/v7+cnZ1Vs2ZNhYWF6b333tOAAQNUpUrJ/pJt0KBBqlatmt5++20999xzql69upo1a6ZRo0YV+Rz5+fl66qmn9Pvvv8vT01Pdu3fXlClTLtu2ZcuW+vLLLzVhwgS9+uqr8vf316RJkwo92HY9tm7dqs6dO9vXL4b2AQMGXDJjBADcCMM0TdPRRQAAAACljTG+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABLIPgCAADAEgi+AAAAsASCLwAAACyB4AsAAABL+P8AR5mWlvXcAXUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['strong',\n",
              " 'is',\n",
              " 'will',\n",
              " 'a',\n",
              " 'be',\n",
              " 'queen',\n",
              " 'young',\n",
              " 'boy',\n",
              " 'woman',\n",
              " 'king',\n",
              " 'man',\n",
              " 'princess',\n",
              " 'pretty',\n",
              " 'wise',\n",
              " 'prince',\n",
              " 'girl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sQ5UWhR_M74"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}