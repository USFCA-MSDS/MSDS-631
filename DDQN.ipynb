{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zayI4AyPmlMR"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('MountainCar-v0')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "env.seed(0)\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Get the state and action dimensions\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "episodes = 100\n",
        "max_steps = 200\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.995\n",
        "batch_size = 64\n",
        "target_update_interval = 10\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define the Double Q-learning agent\n",
        "class DoubleQLearningAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate, gamma):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.model = QNetwork(state_size, action_size)\n",
        "        self.target_model = QNetwork(state_size, action_size)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.memory = deque(maxlen=100000)\n",
        "\n",
        "    def select_action(self, state, epsilon):\n",
        "        if random.random() < epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.model(state_tensor)\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_q_networks(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states_tensor = torch.FloatTensor(states)\n",
        "        actions_tensor = torch.LongTensor(actions)\n",
        "        rewards_tensor = torch.FloatTensor(rewards)\n",
        "        next_states_tensor = torch.FloatTensor(next_states)\n",
        "        dones_tensor = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.model(states_tensor)\n",
        "        next_q_values = self.model(next_states_tensor)\n",
        "        target_next_q_values = self.target_model(next_states_tensor)\n",
        "\n",
        "        q_value_targets = rewards_tensor + self.gamma * target_next_q_values.gather(1, torch.argmax(next_q_values, dim=1, keepdim=True)) * (1 - dones_tensor)\n",
        "\n",
        "        q_value_estimates = q_values.gather(1, actions_tensor.unsqueeze(1))\n",
        "\n",
        "        loss = F.mse_loss(q_value_estimates, q_value_targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "# Create the agent\n",
        "agent = DoubleQLearningAgent(state_size, action_size, learning_rate, gamma)\n",
        "\n",
        "# Create lists to track rewards and epsilon values over episodes\n",
        "rewards = []\n",
        "epsilons = []\n",
        "\n",
        "# Training loop\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Select an action\n",
        "        action = agent.select_action(state, epsilon)\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Store the experience in agent's memory\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        # Update the Q-networks\n",
        "        agent.update_q_networks(batch_size)\n",
        "\n",
        "        # Update the target network periodically\n",
        "        if step % target_update_interval == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Update the state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Track the rewards and epsilon values\n",
        "    rewards.append(total_reward)\n",
        "    epsilons.append(epsilon)\n",
        "\n",
        "    # Print the episode information\n",
        "    print(f\"Episode: {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "# Plot the rewards and epsilon values\n",
        "plt.plot(rewards)\n",
        "plt.title(\"Rewards\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epsilons)\n",
        "plt.title(\"Epsilon\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Epsilon Value\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOfegj_CmpO0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVLrFWMtrixc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}