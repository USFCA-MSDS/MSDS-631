{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Residual block\n",
        "The Residual block, also known as the ResNet block, is a fundamental component in deep neural networks, specifically designed to alleviate the degradation problem encountered in very deep networks. It was introduced in the ResNet architecture, which achieved remarkable performance in image classification tasks.\n",
        "\n",
        "The main concept behind the Residual block is the use of shortcut connections that allow the network to bypass one or more layers, facilitating the flow of information. Unlike traditional network architectures where each layer sequentially transforms the input, Residual blocks introduce skip connections that directly connect the input to the output of the block.\n",
        "\n",
        "The skip connections enable the network to learn residual mappings, capturing the difference between the desired mapping and the identity mapping of the input. This residual information is then added element-wise to the output of the block, effectively allowing the network to fine-tune the learned features and learn more complex representations.\n",
        "\n",
        "By utilizing residual connections, the Residual block addresses the degradation problem, which arises when adding more layers to a network starts to hinder the network's performance. Deep networks often suffer from vanishing gradients or the problem of information degradation as the gradients become increasingly small during backpropagation. The residual connections mitigate this issue by enabling the gradients to flow directly from the output to the input, facilitating the training of deep networks.\n",
        "\n",
        "The Residual block typically consists of two or more convolutional layers, followed by batch normalization and activation functions, such as ReLU (Rectified Linear Unit). The skip connections are implemented as element-wise summation between the input and the output of the block.\n",
        "\n",
        "The introduction of Residual blocks has had a significant impact on deep learning, allowing for the development of much deeper networks with improved performance. Residual architectures have been widely adopted in various domains, including computer vision, natural language processing, and audio processing, and have become a standard building block in state-of-the-art deep neural network architectures."
      ],
      "metadata": {
        "id": "aL_9LCk-DbpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "prp5JcQi5uts"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # Define the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        \n",
        "        # Define the ReLU activation function\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # Define the second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        # Set the stride value\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # Perform the first convolution\n",
        "        out = self.conv1(x)\n",
        "        \n",
        "        # Apply the ReLU activation function\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Perform the second convolution\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        # Adjust the dimensions of the residual if needed\n",
        "        if self.stride != 1 or x.shape[1] != out.shape[1]:\n",
        "            residual = nn.Conv2d(x.shape[1], out.shape[1], kernel_size=1, stride=self.stride, bias=False)(x)\n",
        "\n",
        "        # Add the residual connection\n",
        "        out += residual\n",
        "        \n",
        "        # Apply the ReLU activation function\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a simple CNN architecture using ResidualBlocks\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.res_block1 = ResidualBlock(64, 64)\n",
        "        self.res_block2 = ResidualBlock(64, 64)\n",
        "\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.res_block1(out)\n",
        "        out = self.res_block2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Create an instance of the CNN model\n",
        "model = CNN(num_classes=10)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Generate a random input tensor\n",
        "input_tensor = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor)\n",
        "\n",
        "# Print the output tensor shape\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1tjJomR6GzV",
        "outputId": "b5ace9c6-76a8-4a6c-c9c1-97df13c0a0da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (res_block1): ResidualBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  )\n",
            "  (res_block2): ResidualBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m27xM9s65zdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Inception module\n",
        "The Inception module is a fundamental building block in deep neural networks, primarily used in computer vision tasks such as image classification and object detection. It was introduced in the seminal Inception network, also known as GoogLeNet.\n",
        "\n",
        "The key idea behind the Inception module is to extract features at multiple spatial scales by performing convolutions with different filter sizes simultaneously. Instead of relying on a single convolutional filter size, the module employs a set of parallel convolutional operations, including 1x1, 3x3, and 5x5 convolutions, as well as a 1x1 convolution with max pooling.\n",
        "\n",
        "By combining these operations, the Inception module enables the network to capture both local and global features effectively. It allows for efficient representation learning by reducing the number of parameters while maintaining a large receptive field. Additionally, the 1x1 convolutions within the module aid in dimensionality reduction and can facilitate information flow across different channels.\n",
        "\n",
        "The outputs of the parallel operations within the Inception module are concatenated along the channel dimension and form the input for subsequent layers. This concatenation allows the network to capture diverse features and learn complex representations from the input data.\n",
        "\n",
        "Overall, the Inception module has been highly influential in deep learning and has inspired the development of numerous network architectures. It has significantly contributed to improving the accuracy and efficiency of convolutional neural networks, particularly in the field of computer vision."
      ],
      "metadata": {
        "id": "n36_cpXPDXml"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TVm3Qd0B-T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aIKwYNKsB-ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_1x1, out_3x3_reduce, out_3x3, out_5x5_reduce, out_5x5, out_pool):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        # 1x1 convolution branch\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_1x1, kernel_size=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 3x3 convolution branch\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_3x3_reduce, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_3x3_reduce, out_3x3, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 5x5 convolution branch\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_5x5_reduce, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_5x5_reduce, out_5x5, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Max pooling branch\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, out_pool, kernel_size=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_branch1x1 = self.branch1x1(x)\n",
        "        out_branch3x3 = self.branch3x3(x)\n",
        "        out_branch5x5 = self.branch5x5(x)\n",
        "        out_branch_pool = self.branch_pool(x)\n",
        "\n",
        "        # Concatenate the outputs along the channel dimension\n",
        "        out = torch.cat([out_branch1x1, out_branch3x3, out_branch5x5, out_branch_pool], dim=1)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "jo7PickaB-u0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create an Inception module instance\n",
        "inception_module = InceptionModule(in_channels=256, out_1x1=64, out_3x3_reduce=96, out_3x3=128,\n",
        "                                   out_5x5_reduce=16, out_5x5=32, out_pool=32)\n",
        "\n",
        "# Generate a random input tensor\n",
        "input_tensor = torch.randn(1, 256, 32, 32)\n",
        "\n",
        "# Forward pass through the Inception module\n",
        "output = inception_module(input_tensor)\n",
        "\n",
        "# Print the output tensor shape\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeTZ5BeSCD3l",
        "outputId": "3adf4d6b-9890-45db-efe2-310bb7f7d8b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 256, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_bum5yJCGss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}