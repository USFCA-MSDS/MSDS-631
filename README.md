# MSDS-631
Deep learning is a sub-field of machine learning that focuses on learning complex, hierarchical
feature representations from raw data. Over the past decade, deep learning has been remarkably
successful at solving a massive set of problems on data types including images and sequential data.
This success drove the extension of deep learning to other discrete domains such as sets, point
clouds , graphs, and 3D shapes.


This course introduces the student to a range of topics and concepts in deep learning including
the foundation neural networks, most common neural network architectures such as MLP, convolutional neural networks, and recurrent neural networks to name a few. We will also go over advanced topics such as generative models, geometric deep learning and graph neural networks.
The course covers a practical aspects of deep learning and students get to learn how to use pytroch
for creation/training/ineference of various networks. Intuition, mathematical notions as well as the
practical aspects are all emphasized throughout the course and by the end of the course the student
should have a solid theoerical and practical foundation of deep learning



# Syllabus

* Day 1:
  * Review1 [Linear Algebra for DL](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Linear_algebra_for_DL.ipynb) (notebook)
  * Review2 [Probability for DL](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Introduction_to_Probability_in_Python.ipynb) (notebook)
  * Fundamental algorithm [The gradient descent](https://github.com/USFCA-MSDS/MSDS-631/blob/main/the_gradient_descent_algorithm.ipynb) (notebook)
  * Pytorch [Introduction to Pytorch](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Introduction_to_pytorch-.ipynb) (notebook)
  * A visual introduction to NN [playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.93552&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)
  * How does a DL model transform the input data to the final prediction ? [Youtube video](https://www.youtube.com/watch?v=UOvPeC8WOt8)
  * What is a neural network ? [YouTube](https://www.youtube.com/watch?v=aircAruvnKk)
  * A minimal implementation of a NN [notebook](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Minimal_implementation_of_NN.ipynb)
  * Train your first NN [MLP and dense layers](https://github.com/USFCA-MSDS/MSDS-631/blob/main/MLP_and_Dense_layer.ipynb) (notebook)
  * The maximal likelihood estimation [MLE and its relations to other loss functions](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Maximum_likelihood_estimation%20(2).ipynb) (notebook)
  * Additional : The gradient descent algorithm for dense layers [GD](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Introduction_to_gradient_decent_for_dense_layers_in_the_context_of_binary_classification%20(2).ipynb) (notebook)
  * Additional Stochastic Gradient Decent [Notebook](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Stochastic_Dradient_Descent.ipynb)
* Day 2:
  * Why neural networks can learn anything? [Youtube](https://www.youtube.com/watch?v=0QczhVg5HaI) 
  * The universal approximation theorem for neural network [visual](https://gaoxiangluo.github.io/2020/09/27/Visual-and-Rigorous-Proof-of-Universal-Approximation-Theorem-UAT/)
  * A minimal introduction to automatic differentiation [Notebook](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Introduction_to_auto_diff_in_pytorch.ipynb)
  * 
  * Introduction to conv and pooling layers using Numpy [Notebook](https://github.com/USFCA-MSDS/MSDS-631/blob/main/Introduction_to_pooling_and_cov_layers_using_numpy.ipynb)
  * An introduction to CNNs [MNIST example](https://github.com/USFCA-MSDS/MSDS-631/blob/main/AlexNet.ipynb) (notebook)
